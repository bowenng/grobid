<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<head>I<lb/></head>

			<p>nternational inequalities have the potential to disrupt the gen-<lb/>eration and flow of knowledge in the global scientific commu-<lb/>nity. Research can make an impact only if it is visible. Of course,<lb/> journals and scientific reputations shape the visibility of research<lb/> in conjunction with the overall quality of a study. But so too do<lb/> national scientific infrastructures and reputations. The scale and<lb/> quality of research, and even library subscription practices, are<lb/> heavily influenced by the amount of funding countries set aside<lb/> for the scientific enterprise <ref type="biblio">1-4</ref> . Countries with these advantages<lb/> probably receive additional citations for their research, over and<lb/> above what one would expect just from the subject matter of that<lb/> research <ref type="biblio">5</ref> , something that scholars of the Global South have sug-<lb/>gested before <ref type="biblio">2,6,7</ref> . Studying these inequalities systematically is diffi-<lb/>cult because the many relevant factors are so deeply intertwined. In<lb/> this paper, we introduce a framework we call citational lensing that<lb/> takes advantage of the strong relationship between citations and<lb/> textual similarity to identify countries that receive more (or fewer)<lb/> citations than one would expect if citations and textual similarity<lb/> were perfectly aligned.<lb/></p>

			<p>Much like how gravity distorts our perception of light, national<lb/> factors distort our perception of international science. The intu-<lb/>ition behind our approach is to think of international science as a<lb/> multiplex network, with the citations running between countries<lb/> representing one type of connection, and the textual similarity of<lb/> their research output representing another. Citational lensing is<lb/> measured as the difference between the weighted edges that cor-<lb/>respond to international citations and the weighted edges that cor-<lb/>respond to textual similarity. The result is a set of connections that<lb/> represent how much more one country cites another, relative to<lb/> what we would expect if citations and textual similarity reflected<lb/> each other perfectly. We call this layer of the multiplex network<lb/> the citational well. Filtering out the effect of other factors is also<lb/> possible, leaving distortion captured in the citational well to repre-<lb/>sent a more narrowly defined notion of inequality. This is shown in<lb/> Fig. <ref type="figure">1</ref>, where we represent science as a multiplex network with three<lb/> layers: L citation is the citation network between countries containing<lb/> the citation flow from country i to country j, L T<lb/> text is the relative sim-<lb/>ilarity of the text of country j&apos;s research output to that of country i<lb/> by applying a Kullback-Leibler divergence (KLD) measure <ref type="biblio">8</ref> to the<lb/> distinct national signatures of countries i and j produced by a super-<lb/>vised topic model called a labelled latent Dirichlet allocation (LDA)<lb/> model <ref type="biblio">9</ref> , and L distortion is what we call the citational well defined as the<lb/> difference between the two layers:<lb/></p>

				<formula>L distortion = L citation -L T<lb/> text<lb/> (1)<lb/></formula>

			<p>L distortion represents the distortion in the citations from country i<lb/> to country j, relative to what we would expect given the similarity of<lb/> the research written by scientists in these two countries.<lb/></p>

			<p>Our approach builds on the long-standing tradition in the sci-<lb/>ence of science that uses citation networks and text analysis of<lb/> scientific papers to embody the flow of ideas in science and map<lb/> its structure, as well as the distribution and spread of knowledge<lb/> within it <ref type="biblio">5,10-14</ref> . Yet the citation networks and the textual similarity<lb/> between fields are not always aligned. There are commonly more<lb/> citations between fields than we would expect on the basis of the<lb/> textual similarity of their papers, or conversely, more similarity in<lb/> the text than we would expect given the number of citations flowing<lb/> between those fields <ref type="biblio">12,15</ref> .<lb/></p>

			<p>It is not clear whether this misalignment between citations and<lb/> textual similarity has any substantive importance for scientific<lb/> fields <ref type="biblio">16</ref> . Bibliometrics has treated the issue as a question of ground<lb/> truth, where the differences between the two are less important than<lb/> their respective differences from a third, external criterion <ref type="biblio">17,18</ref> . In<lb/> this line of thinking, the misalignment of citations and textual simi-<lb/>larity is simply beside the point and does not impact the larger goal<lb/> of mapping science. In the science of science, meanwhile, the mis-<lb/>alignment is taken as a sign that any model of diffusion or commu-<lb/>nication between scientific fields needs to take both citations and<lb/> textual similarity into account <ref type="biblio">11,12,15</ref> .<lb/></p>

			<p>When it comes to countries, misalignments between citations<lb/> and textual similarity carry practical significance. This is because<lb/></p>

			<head>Articles<lb/></head>

			<p>NATure HuMAN BeHAviOur<lb/> they represent the combined effect of several factors, including<lb/> the overall quality of research and national reputations. The cita-<lb/>tional well provides a noisy signal that can be further refined to<lb/> better approximate specific factors, as we show with respect to<lb/> national reputation.<lb/></p>

			<p>Identifying citational distortions is critical to discussions on<lb/> inequality in global scientific knowledge production. Knowledge<lb/> production is overwhelmingly skewed towards resource-wealthy<lb/> countries such as the United States and those in Western Europe<lb/> and East Asia (to name a few) that house the best universities,<lb/> Nobel winners and journal editors, so identifying undercited coun-<lb/>tries both promotes the inclusion of often-excluded voices and<lb/> helps foster the scientific enterprises of these countries. The type of<lb/> distortion we consider here is also likely to be problematic for sci-<lb/>entific progress if knowledge remains unincorporated and human<lb/> capital unused.<lb/></p>

			<head>results<lb/></head>

			<p>To illustrate how citational lensing can be applied, we use roughly<lb/> 20 million academic papers in nearly 150 fields and subfields<lb/> from 1980 to 2012 in the Microsoft Academic Graph (MAG), one<lb/> of the most extensive metadata repositories of academic publica-<lb/>tions. These data include metadata such as citations, along with the<lb/> abstract text of published research articles. We show how citational<lb/> lensing can be used to characterize changes in the international<lb/> scientific hierarchy over time and how it can be scaled to cover all<lb/> of science.<lb/></p>

			<head>Citations and recognition.</head>

			<p>In Fig. <ref type="figure">2a</ref>, we count for each year the<lb/> number of countries that are present in both the international text<lb/> similarity network and the international citation network for each<lb/> field and for each year from 1980 to 2012. The plot captures the<lb/> distribution of fields in each year, as well as the overall average<lb/> number of countries represented in fields for each year. Across all<lb/> types of fields, the number of countries represented in the global<lb/> scientific conversation is increasing, as captured by journal arti-<lb/>cles. Note that while Fig. <ref type="figure">2a</ref> shows steady growth in the number<lb/> of countries in the international scientific community over time,<lb/> this appears to taper off in the years immediately prior to 2012. In<lb/> Supplementary Tables <ref type="table">3 and 4</ref>, we test how much variance in our<lb/> citational distortion and thus our text similarity measures is due to<lb/> the result of the sheer volume of papers produced by authors from<lb/> countries using hierarchical linear models. The mediating impact<lb/> of N papers is modest. Adjusting for N papers reduces the vari-<lb/>ance in country-related text similarity by 19%, from 0.16 to 0.13.<lb/> Likewise, adjusting for N papers reduces the country-related vari-<lb/>ance in citational distortion by 12.5%, from 0.08 to 0.07.<lb/></p>

			<p>While there may be more countries participating in global sci-<lb/>ence, the extent to which their work is visible and valued may still be<lb/> highly stratified. The relationship between the textual similarity of<lb/> countries&apos; research output and the number of citations they receive<lb/> is one simple way to think about this. In a world where all research<lb/> is equally visible and equally valued, we would expect an extremely<lb/> close relationship between the similarity of research in their text and<lb/> the citations that flow between them. We test the extent to which<lb/> L citation corresponds with L text across fields and over time in Fig. <ref type="figure">2b</ref>.<lb/> This is done using a form of network regression model called the<lb/> semi-partialing quadratic assignment procedure (QAP) <ref type="biblio">19</ref> to capture<lb/> the extent to which citations are associated with L T<lb/> text . Specifically,<lb/> we run two types of QAP models based on which country nodes are<lb/> included in L text and L citation , the first using all countries present in the<lb/> data that year and the second using only &apos;core&apos; scientific countries-<lb/>specifically, those in Western Europe and East Asia (that is, China,<lb/> Japan and South Korea), alongside the United States, Canada,<lb/> Australia, New Zealand, Singapore and Israel. (In Supplementary<lb/> Table <ref type="table">2</ref>, we itemize every country we use in our analysis, parsed by<lb/> their core and periphery classification.) These countries are com-<lb/>monly considered to be the leaders in scientific research (housing<lb/> the best universities and most-cited scientists, publishing work in<lb/> leading outlets and so on) and as such form the core of global sci-<lb/>entific communities where most scientific activity takes place <ref type="biblio">20,21</ref> .<lb/></p>

			<p>We run each type of QAP model for every field in every year,<lb/> from 1980 to 2012. The dependent variable in each model is the<lb/> network represented in L T<lb/> text for that field and year, while the inde-<lb/>pendent variable is the international citation network for that field<lb/> and year, given as L citation . We thus regress how similar country j is to<lb/> country i in L T<lb/> text on how much country j cites country i. There are<lb/> two &apos;grand average&apos; trends in Fig. <ref type="figure">2b</ref>: the average β coefficient value<lb/> across all fields in each year, and the shaded area in the plot, which<lb/> is the grand standard errors across all βs in each year. Note that<lb/> we only consider βs that are statistically significant at a two-tailed<lb/> P value threshold of 0.05.<lb/></p>

			<p>There are a few notable take-aways from these trends. A<lb/> one-standard-deviation increase in the number of citations, as<lb/> defined for each QAP model in the year 2012, is associated with<lb/> 0.228-standard-deviation-higher KLD scores in L text among all<lb/> countries (that is, an all-inclusive model with both core and periph-<lb/>ery countries) (N = 135; 95% confidence interval (CI), (0.226,<lb/> 0.231); two-tailed t-tests). However, when we compare the QAP<lb/> model with just core countries against the QAP model that includes<lb/> all countries (that is, core and periphery), we find that citations<lb/> have a consistent and strong relationship with the similarity of lan-<lb/>guage in international research, where a one-standard-deviation<lb/> increase in the number of citations in 2012 is associated with<lb/> 0.312-standard-deviation-higher KLD scores in L text for just core<lb/> countries (N = 139; 95% CI, (0.309, 0.315); two-tailed t-tests). This<lb/> is also evidenced by the overall higher average β values that remain<lb/> fairly steady over time, albeit with a slight increase in the same<lb/> period within the core-country QAP model. The average β value for<lb/> the core-periphery model, by contrast (which reflects the relation-<lb/>ship across all countries in the dataset), is not only lower overall than<lb/> that for the core country model but is also weakening over time.<lb/> (In Supplementary Figs. <ref type="figure">8 and 9</ref>, we rerun the QAP models to now<lb/> include countries&apos; β coefficients that include only peripheral coun-<lb/>tries, similar to the core network that contains only core countries.)<lb/></p>

			<p>Figure <ref type="figure">3</ref> plots the average citational distortion (that is, the aver-<lb/>age in-degree centrality) that each country experiences in L distortion .<lb/> This should be interpreted as the difference in the number of stan-<lb/>dard deviations between edges in the citation network (measured in<lb/></p>

			<figure>i<lb/> i<lb/> j<lb/> L citation<lb/> j<lb/> L distortion<lb/> L T<lb/> text<lb/> i<lb/> j<lb/> Fig. 1 | the construction of the multiplex network, the citational well.<lb/></figure>

			<p>The citational well is the result of subtracting the edge weights of the<lb/> international text similarity network from the corresponding edge weights<lb/> of the international citation network.<lb/></p>

			<p>terms of standard deviations relative to the citation network) and<lb/> those in the text similarity network (measured in terms of standard<lb/> deviations relative to the text similarity network). Figure <ref type="figure">3a</ref> high-<lb/>lights many of the countries with the greatest positive distortions.<lb/> Figure <ref type="figure">3b</ref> shows the average citational distortion over time among<lb/> core countries and among periphery countries. (As the United<lb/> States is an outlier, we remove it from the average core trend in<lb/> Fig. <ref type="figure">3b</ref> and all subsequent in-degree plots.) The United States is the<lb/> most central country in the citation networks, for all fields and over<lb/> time. The deviance of the United States&apos; centrality in L distortion sug-<lb/>gests that it is, on average, highly overcited. This holds true for other<lb/> power players in global science, such as Germany, the Netherlands,<lb/> the United Kingdom and Japan. The other major trend is that China<lb/> rises considerably over the past few decades, from being undercited<lb/> throughout the 1980s and early 1990s to being overcited in the<lb/> 2000s, even quickly approaching many countries in Western Europe.<lb/></p>

			<p>As shown in Fig. <ref type="figure">3b</ref>, the gap between core and periphery countries<lb/> is growing substantially over time, as core countries are increasingly<lb/> overcited for their work relative to what they study, while peripheral<lb/> countries are increasingly undercited for their work. (As the grow-<lb/>ing gap between the trends in core and periphery countries may<lb/> be the result of the number of countries, in Supplementary Figs. <ref type="biblio">28<lb/></ref> and 29, we censor them on the basis of when countries first appeared<lb/> in the data for each field, finding that our results still hold.)<lb/></p>

			<figure type="table">20<lb/> 40<lb/> 60<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> Year<lb/> Number of countries<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> a<lb/> 0.24<lb/> 0.26<lb/> 0.28<lb/> 0.30<lb/> 0.32<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> Year<lb/> β coefficient<lb/> (standard deviations)<lb/> Core + periphery<lb/> Core<lb/> b<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> 0.20<lb/> 0.25<lb/> 0.30<lb/> 0.35<lb/> Year<lb/> β coefficient<lb/> (standard deviations)<lb/> Core + periphery<lb/> Core<lb/> c<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/></figure>

			<figure>Fig. 2 | relationship between citations and text over time. a, The average number of countries present in both the international text similarity network and<lb/> the international citation network over time from 1980 to 2012, with trend lines for each field type. b, The average of statistically significant β coefficients<lb/> from each field&apos;s yearly QAP model for citations plotted on the y axis and over time on the x axis from 1980 to 2012. The shading around the trends<lb/> denotes the standard errors across fields. c, The trends plotted in b, but parsed by the type of field. The shading around the trends denotes the grand<lb/> standard errors across β coefficients.<lb/> Figure 4a expands the plots in Fig. 3a, while Fig. 4b expands the<lb/> plots in Fig. 3b, but the trends are now averaged by research area.<lb/></figure>

			<p>While the most overcited countries are still present across fields, the<lb/> ordering differs. The United States remains the most central across<lb/> all fields. China, however, has overtaken some European countries<lb/> in the physical and mathematical sciences and in the engineering<lb/> and computational sciences. In the biomedical, behavioural and eco-<lb/>logical sciences, China is nearing Germany but trailing the United<lb/> Kingdom. The exception is the social sciences, where China is on<lb/> par with Germany but noticeably trails the United Kingdom. When<lb/> comparing core and periphery countries in Fig. <ref type="figure">4b</ref> by research area,<lb/> the gap is most pronounced in the physical and mathematical sci-<lb/>ences, followed by the engineering and computational sciences and<lb/> the biomedical, behavioural and ecological sciences. The gap has<lb/> only recently emerged in the social sciences.<lb/></p>

			<p>A stagnating hierarchy. Thus far, we have focused on the most<lb/> overcited distorted countries across different fields and over time.<lb/> However, most countries are undercited. With some exceptions,<lb/> countries that are overcited or undercited remain constant over<lb/> time. To unpack this, Fig. <ref type="figure">5</ref> plots the average distortion for several<lb/> countries at two points in time-the year 2000 on the x axis and<lb/> the year 2012 on the y axis-but parsed by research area and in<lb/> four transnational regions: (1) Europe, (2) Asia, (3) Africa and the<lb/> Middle East and (4) Latin America and the Caribbean. First, with-<lb/>out any divides by research area or transnational region as shown<lb/> here, all countries cluster very closely near the y = x parity line<lb/> (with a Pearson&apos;s product-moment correlation of ρ = 0.659; t = 9.59;<lb/> d.f. = 120; 95% CI, (0.545, 0.749); P &lt; 2.2 × 10 -16 ). Countries tend to<lb/> cluster in one of two quadrants: overcited in both 2000 and 2012<lb/> (that is, data points that are positive in both years) or undercited<lb/> in both 2000 and 2012 (that is, data points that are negative in both<lb/> years). Overcited countries are typically part of the core of global<lb/> science. There are far fewer countries in the overcited quadrant than<lb/> in the undercited quadrant, which comprises the periphery of global<lb/> science. Except the power players in global science, most countries<lb/> seem to be under-recognized for their work. In other words, most<lb/> countries remain in either the lower left quadrant or the upper right<lb/> quadrant, indicating that countries do not generally change their<lb/> station in their distortion over time.<lb/></p>

			<p>That being said, note as well that some regions and research areas<lb/> are more stable than others in terms of the average citational distor-<lb/>tion for countries across the two periods. Unsurprisingly, Europe<lb/> shows high correlations for all research areas (ρ = 0.68; t = 5.01;<lb/> d.f. = 29; 95% CI, (0.431, 0.834); P &lt; 2.47 × 10 -5 for biomedical,<lb/> behavioural and ecological sciences; ρ = 0.66, t = 5.00; d.f. = 32;<lb/> 95% CI, (0.418, 0.817); P &lt; 1.99 × 10 -5 for engineering and compu-<lb/>tational sciences; ρ = 0.76; t = 7.13; d.f. = 37; 95% CI, (0.586, 0.868);<lb/> P &lt; 1.92 × 10 -8 for physical and mathematical sciences; ρ = 0.66;<lb/> t = 4.27; d.f. = 23; 95% CI, (0.366, 0.839); P &lt; 2.89 × 10 -4 for social<lb/> sciences). Other regions show more variation across research areas.<lb/> While citational distortions in engineering and computational sci-<lb/>ences are highly correlated in Asia, with lower correlations in other<lb/> research areas, Latin America and the Caribbean show a different<lb/> pattern, with a high correlation in the biomedical, behavioural and<lb/> ecological sciences and lower correlations in other research areas.<lb/> By contrast, Africa and the Middle East have a lower (albeit sta-<lb/>tistically significant) correlation in the engineering and computa-<lb/>tional sciences, but lower and non-significant correlations in other<lb/> research areas.<lb/></p>

			<figure>0<lb/> 0.5<lb/> 1.0<lb/> 1.5<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> 2020<lb/> Year<lb/> Average citational distortion<lb/> (standard deviations)<lb/> a<lb/> Average citational distortion<lb/> (standard deviations)<lb/> b<lb/> 0.05<lb/> 0<lb/> 0.05<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> Year<lb/> Periphery<lb/> Core<lb/> China<lb/> Germany<lb/> Japan<lb/> Netherlands<lb/> Switzerland<lb/> United Kingdom<lb/> United States<lb/> Fig. 3 | Comparing global citational distortion over time. a, The average national citational distortion in L distortion plotted across fields on the y axis and over<lb/> time on the x axis for select countries. b,</figure>

			<p>The average national citational distortion plotted for core countries and peripheral countries on the y axis and over<lb/> time on the x axis. The gap in the average distortion in the citational well is growing between core and periphery countries. The shading around the trends<lb/> denotes the standard errors of these averages. Note that citational distortion should be interpreted as the difference in the number of standard deviations<lb/> between edges in the citation network (measured in terms of standard deviations relative to the edge weights in the citation network) and those in the text<lb/> similarity network (measured in terms of standard deviations relative to the edge weights in the text similarity network).<lb/></p>

			<p>Figure <ref type="figure">6</ref> plots the percentage of countries in the core and in the<lb/> periphery that are being overcited or undercited in the year 2000<lb/> and in the year 2012 for different types of fields. Here the denomi-<lb/>nator is the number of countries present in the year 2000 or the year<lb/> 2012, where summing the percentages vertically in each year results<lb/> in 100%. The data points in the figure also contain the number of<lb/> countries (given as N) and the average distortion value with its stan-<lb/>dard errors in parentheses.<lb/></p>

			<p>Except for the physical and mathematical sciences, where the<lb/> representation of countries in the overcited and undercited groups<lb/> remained steady, the percentage of core countries that were over-<lb/>cited increased from 2000 to 2012. For the biomedical, behavioural<lb/> and ecological sciences, the percentage increased from 9.68% of all<lb/> countries in 2000 to 22.73% of all countries in 2012; for the engi-<lb/>neering and computational sciences, from 13.64% of all countries<lb/> to 22.73% of all countries; and for the social sciences, from 2.27%<lb/> of all countries to 9.09% of all countries. Similarly, the percentage<lb/> of periphery countries that were undercited rose over the same<lb/> period. For the biomedical, behavioural and ecological sciences,<lb/> the percentage increased from 40.32% of all countries in 2000 to<lb/> 43.55% of all countries in 2012; for the engineering and compu-<lb/>tational sciences, from 39.83% of all countries to 45.76% of all<lb/> countries; and for the social sciences, from 36.9% of all countries<lb/> to 42.86% of all countries. Notice as well that the changing repre-<lb/>sentation of countries within the overcited and undercited groups<lb/> does not necessarily correspond to the average distortion numbers<lb/> within those groups.<lb/></p>

			<p>Figure <ref type="figure">7</ref> uses the average distortion of each country in 2012 by<lb/> field type to calculate transnational regional averages to include<lb/> the four regions mentioned previously, as well as North America<lb/> (the United States and Canada) and Oceania (notably including<lb/> Australia and New Zealand). This figure was plotted in R using<lb/> the package maps <ref type="biblio">22</ref> . We created global maps based on the average<lb/> distortion of countries within transnational regions in the year<lb/> 2012 for the different types of fields. Figure <ref type="figure">8</ref> expands this map<lb/> by plotting the average national distortion within each region and<lb/> parsed by the type of field. This figure was also plotted using the<lb/> package maps <ref type="biblio">22</ref> . Note that for the transnational regional figures in<lb/> Fig. <ref type="figure">8a-d</ref>, the colour scales are mapped for the data of countries in<lb/> those regions. Countries should not be compared in their colours<lb/> across figures. The global map in Fig. <ref type="figure">7</ref> highlights the main regional<lb/> differences for that purpose. There are several notable countries in<lb/> each region, such as China in Fig. <ref type="figure">8b</ref> and Brazil in Fig. <ref type="figure">8c</ref>. In con-<lb/>trast, countries in Africa and the Middle East and most countries<lb/> in South America are near parity, where they are cited to the same<lb/> degree to which their research language aligns to other countries.<lb/></p>

			<figure>China<lb/> Germany<lb/> Japan<lb/> Netherlands<lb/> Switzerland<lb/> United Kingdom<lb/> United States<lb/> China<lb/> Germany<lb/> Japan<lb/> Netherlands<lb/> Switzerland<lb/> United Kingdom<lb/> United States<lb/> China<lb/> Germany<lb/> Japan<lb/> Netherlands<lb/> Switzerland<lb/> United Kingdom<lb/> United States<lb/> China<lb/> Germany<lb/> Japan<lb/> Netherlands<lb/> Switzerland<lb/> United Kingdom<lb/> United States<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 1980<lb/> 2000<lb/> 2020<lb/> 1980<lb/> 2000<lb/> 2020<lb/> 1980<lb/> 2000<lb/> 2020<lb/> 1980<lb/> 2000<lb/> 2020<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> 1.5<lb/> Year<lb/> Average citational distortion<lb/> (standard deviations)<lb/> a<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010 1980<lb/> 1990<lb/> 2000<lb/> 2010<lb/> 0.10<lb/> 0.05<lb/> 0<lb/> 0.05<lb/> 0.10<lb/> 0.15<lb/> Year<lb/> Average citational distortion<lb/> (standard deviations)<lb/> Periphery<lb/> Core<lb/> b<lb/> Fig. 4 | Comparing global citational distortion by the type of field. a,b, The trends plotted in Fig. <ref type="figure">3</ref>, but parsed by the type of field. The shading around<lb/> the trends denotes the standard errors of these averages. Note that citational distortion should be interpreted as the difference in the number of standard<lb/> deviations between edges in the citation network (measured in terms of standard deviations relative to the edge weights in the citation network) and those<lb/> in the text similarity network (measured in terms of standard deviations relative to the edge weights in the text similarity network).<lb/></p>

			<head>Articles<lb/> NATure HuMAN BeHAviOur<lb/> Discussion<lb/></head>

			<p>These results are especially interesting in light of the broader lit-<lb/>erature. The rise of China, for instance, has been noted with other<lb/> bibliometric data <ref type="biblio">23-25</ref> , so for the country to also be experiencing an<lb/> increase in citational distortion adds another element to this his-<lb/>tory. However, our results do not similarly re-affirm the temporal<lb/> trends around Europe in recent work on citation inequality among<lb/> elite researchers. Where that work showed that countries such as<lb/> the Netherlands and Switzerland have a large and increasing share<lb/> of elite researchers <ref type="biblio">5</ref> , Fig. <ref type="figure">3a</ref> shows that while the Netherlands and<lb/> Switzerland are increasingly reaping citations in excess of the textual<lb/> similarity of their research, they fall far closer to the mean. Finally, it<lb/> is noteworthy that the gap between countries with high levels and low<lb/> levels of citational distortion is most pronounced in the physical sci-<lb/>ences, considering that these fields are traditionally known for shar-<lb/>ing the strongest sense of how to evaluate and integrate knowledge <ref type="biblio">26</ref> .<lb/></p>

			<p>The main limitation of the citational lensing framework is one of<lb/> measurement. Textual similarity between countries is an unavoid-<lb/>ably noisy signal, and this affects the comparison with citations<lb/> downstream. So, even though it is correct to say that the international<lb/> inequalities revealed in our analyses are a matter of prominence and<lb/> recognition, more precision will come only with future refinements<lb/> to the methodology. We have been able to correct for citation infla-<lb/>tion in Supplementary Figs. <ref type="figure">10-15</ref>, and in Supplementary Figs. <ref type="figure">16-21<lb/></ref> we describe the results of a secondary analysis where we test our<lb/> rudimentary controls for the quality of research by uncensoring<lb/> journal selection. However, there are other factors that matter and<lb/> could always be taken into account in the future.<lb/></p>

			<p>These potential concerns are offset by a number of unique<lb/> strengths that the citational lensing framework provides. One of<lb/> the primary ones we have outlined is its adaptability. We have used<lb/> nation-labelled LDA (NL-LDA) in tandem with the KLD to model<lb/> the similarity of scientific text, but many other approaches could<lb/> be used in their place to capture another nuance around language<lb/> use in science. The entropy-based metrics advocated by Vilhena<lb/> et al. <ref type="biblio">12</ref> and Altmann et al. <ref type="biblio">13</ref> would bring more attention to ineffi-<lb/>ciencies in international communication among scientists, to take<lb/> one example.<lb/></p>

			<figure>R = 0.32<lb/> P = 0.25<lb/> R = 0.75<lb/> P = 0.0014<lb/> R = 0.52<lb/> P = 0.034<lb/> R = 0.14<lb/> P = 0.66<lb/> R = 0.68<lb/> P = 2.5 × 10 -5<lb/> R = 0.66<lb/> P = 2 × 10 -5<lb/> R = 0.76<lb/> P = 1.9 × 10 -8<lb/> R = 0.66<lb/> P = 0.00029<lb/> R = 0.75<lb/> P = 0.0045<lb/> R = 0.27<lb/> P = 0.52<lb/> R = 0.46<lb/> P = 0.13<lb/> R = 0.4<lb/> P = 0.28<lb/> R = -0.16<lb/> P = 0.42<lb/> R = 0.46<lb/> P = 0.025<lb/> R = 0.25<lb/> P = 0.19<lb/> R = 0.27<lb/> P = 0.27<lb/> Asia<lb/> Europe<lb/> Latin America<lb/> and the Caribbean<lb/> Africa and the<lb/> Middle East<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 1.0<lb/> 0.5<lb/> 0<lb/> 0.5 1.0<lb/> 0.5<lb/> 0<lb/> 0.5 1.0<lb/> 0.5<lb/> 0<lb/> 0.5 1.0<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> Average citational distortion (standard deviations)<lb/> 2000<lb/> Average citational distortion (standard deviations)<lb/> 2012<lb/> Fig. 5 | Comparing citational distortion between 2000 and 2012. The average distortion for countries in the year 2012, parsed by region and by the type<lb/> of discipline. The trends for all countries in the years 2000 and 2012 are plotted on the x axis and y axis, respectively, but parsed by the type of field and by<lb/> transnational region. R refers to the Pearson&apos;s product-moment correlation.<lb/> Articles<lb/> NATure HuMAN BeHAviOur<lb/> 2000<lb/> 2012<lb/> 2000<lb/> 2012<lb/> 2000<lb/> 2012<lb/> 2000<lb/> 2012<lb/> 5.0<lb/> 10.0<lb/> 15.0<lb/> 20.0<lb/> 30.0<lb/> 35.0<lb/> 40.0<lb/> 45.0<lb/> Percentage of countries (%)<lb/> Periphery<lb/> Core<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> Overcited<lb/> Undercited<lb/> 9.09% N = 4<lb/> 0.13 (0.04)<lb/></figure>

			<figure type="table">9.68% N = 12<lb/> 0.13 (0.04)<lb/> 22.73% N = 10<lb/> 0.13 (0.05)<lb/> 6.45% N = 8<lb/> 0.14 (0.07)<lb/> 40.91% N = 18<lb/> 0.1 (0.02)<lb/> 40.32% N = 50<lb/> 0.17 (0.02)<lb/> 27.27% N = 12<lb/> 0.07 (0.01)<lb/> 43.55% N = 54<lb/> 0.15 (0.01)<lb/> 13.64% N = 6<lb/> 0.11 (0.04)<lb/> 10.17% N = 12<lb/> 0.12 (0.04)<lb/> 22.73% N = 10<lb/> 0.18 (0.11)<lb/> 4.24% N = 5<lb/> 0.08 (0.07)<lb/> 36.36% N = 16<lb/> 0.08 (0.01)<lb/> 39.83% N = 47<lb/> 0.19 (0.02)<lb/> 27.27% N = 12<lb/> 0.09 (0.01)<lb/> 45.76% N = 54<lb/> 0.16 (0.02)<lb/> 18.18% N = 8<lb/> 0.23 (0.06)<lb/> 10.14% N = 15<lb/> 0.15 (0.04)<lb/> 18.18% N = 8<lb/> 0.3 (0.12)<lb/> 10.14% N = 15<lb/> 0.09 (0.03)<lb/> 31.82% N = 14<lb/> 0.12 (0.02)<lb/> 39.86% N = 59<lb/> 0.17 (0.02)<lb/> 31.82% N = 14<lb/> 0.06 (0.01)<lb/> 39.86% N = 59<lb/> 0.17 (0.02)<lb/> 2.27% N = 1<lb/> 0.03 (NA)<lb/> 13.1% N = 11<lb/> 0.12 (0.04)<lb/> 9.09% N = 4<lb/> 0.15 (0.05)<lb/> 7.14% N = 6<lb/> 0.17 (0.1)<lb/> 47.73% N = 21<lb/> 0.08 (0.01)<lb/> 36.9% N = 31<lb/> 0.19 (0.02)<lb/> 40.91% N = 18<lb/> 0.08 (0.01)<lb/> 42.86% N = 36<lb/> 0.13 (0.01)<lb/></figure>

			<head>Year<lb/></head>

			<p>Fig. <ref type="figure">6</ref> | Comparing citational distortion between core and periphery countries. The percentage of countries in the core or periphery that are either overcited<lb/> or undercited in the years 2000 and 2012, along with the average distortion for each group (standard errors are given in parentheses). Note: NA means<lb/> not available.<lb/></p>

			<figure>Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> 0<lb/> 0.2<lb/> 0.4<lb/> 0.6<lb/> Average citational<lb/> distortion (standard deviations)<lb/> Fig. 7 | Comparing citational distortion by transnational region in 2012. The average distortion of countries mapped within transnational regions-Africa<lb/> and the Middle East, Latin America and the Caribbean, Asia, North America, and Oceania-and by the type of field.<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 0.3<lb/> 0<lb/> 0.3<lb/> 0.6<lb/> Average citational<lb/> distortion (standard<lb/> deviations)<lb/> a<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 0.5<lb/> 0<lb/> 0.5<lb/> 1.0<lb/> Average citational<lb/> distortion (standard<lb/> deviations)<lb/> b<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 0.50<lb/> 0.25<lb/> 0<lb/> 0.25<lb/> Average citational<lb/> distortion (standard<lb/> deviations)<lb/> c<lb/> Biomedical, behavioural<lb/> and ecological sciences<lb/> Engineering and<lb/> computational sciences<lb/> Physical and<lb/> mathematical sciences<lb/> Social<lb/> sciences<lb/> 0.50<lb/> 0.25<lb/> 0<lb/> 0.25<lb/> 0.50<lb/> Average citational<lb/> distortion (standard<lb/> deviations)<lb/> d<lb/> Fig. 8 | Citational distortion within transnational regions in 2012. a-d, The average distortion for individual countries in Europe (a), Asia (b), Latin<lb/> America and the Caribbean (c), and Africa and the Middle East (d) in 2012. Note that the colour scale varies by region to highlight regional trends that are<lb/> not visible when using a global colour scale.<lb/></figure>

			<head>Articles<lb/> NATure HuMAN BeHAviOur<lb/></head>

			<p>The citational well may also be meaningful in contexts where<lb/> nation-states are not the main feature of interest. Reputation effects<lb/> among journals or universities, for instance, could be studied with<lb/> the method we have described. The comparison may also be rele-<lb/>vant to questions surrounding innovation and intellectual property,<lb/> as patents are themselves embedded within a citation network and<lb/> can be evaluated on the similarity of their language use. Any exten-<lb/>sion will have to be validated, but the simplicity of the method lends<lb/> itself well to re-use in new contexts.<lb/></p>

			<p>Regardless, identifying countries that do not receive citations<lb/> in the amount we would expect given the subject matter of their<lb/> research-whether they receive more citations or fewer-provides<lb/> us with a tool to check on the efficiency of knowledge flow in sci-<lb/>ence. Indeed, as demonstrated in Figs. <ref type="figure">3 through 8</ref>, only a few<lb/> highlighted countries are the real sole winners here, as the overall<lb/> average β coefficient trend plotted in Fig. <ref type="figure">2b</ref> and the overall average<lb/> citational distortion trend plotted in Fig. <ref type="figure">3b</ref> are quite modest over<lb/> 30 years. This holds real value for the study of scientific research. In<lb/> the first place, citational lensing at the international level makes it<lb/> possible to identify countries with a successful scientific enterprise.<lb/> Citations should not be the sole arbiter for success in science, though<lb/> the reality is that they continue to be disproportionately impactful<lb/> as a metric. Countries that seem overcited by the approach we have<lb/> taken here have been successful in this narrow sense. The empiri-<lb/>cal results bear out these points. Consider the United States and<lb/> China: both countries reap far more citations than the language of<lb/> their work would suggest. While this has been true for the United<lb/> States for a long time, reflective of its large economy and substantial<lb/> investment in scientific research, China&apos;s success in attracting cita-<lb/>tions has come more recently. More work would be needed to estab-<lb/>lish any causal connection, but the timing of China&apos;s rise in Figs.<lb/> 3a, 4a and 8b is conspicuous given its major policy shift regarding<lb/> science and technology in the 1990s.<lb/></p>

			<p>This leaves citational lensing as a useful metric in tracking the<lb/> effectiveness of national science policies, as well as evaluating the<lb/> relative importance of various national factors in nurturing a highly<lb/> cited scientific community. Otherwise, citational distortions will<lb/> continue to impose limits on the circulation of knowledge, novel<lb/> ideas and future innovations and are ultimately inefficient for sus-<lb/>tained knowledge production. Better identifying who is undercited<lb/> not only promotes the inclusion of often excluded perspectives but<lb/> also enhances knowledge production. The exposure to diverse per-<lb/>spectives and abilities consistently improves outcomes in collective<lb/> problem-solving ventures such as scientific research collabora-<lb/>tions <ref type="biblio">27</ref> . At the very least, overlooking research from wide swaths<lb/> of the global scientific community means that knowledge remains<lb/> unincorporated and human capital unused, especially in many ris-<lb/>ing middle-income countries with growing scientific enterprises.<lb/></p>

			<p>Citational lensing also offers a way to study the differences in<lb/> knowledge production between the Global South and the Global<lb/> North, and the flow of knowledge between them. The long-standing<lb/> concerns around the global inequalities in science have typically<lb/> been substantiated with analyses of publication patterns or with rich<lb/> qualitative interviews <ref type="biblio">2,28-31</ref> . By bringing attention to the (mis)match<lb/> between citations and textual similarity, citational lensing can help<lb/> reveal more of the true impact that countries imprint on discourses<lb/> across scientific fields and on the disproportionate attention and<lb/> recognition that some research receives. Progress has been made by<lb/> several countries in the semi-periphery towards this end, but each<lb/> new entrant to the international scientific community continues to<lb/> face a struggle in getting the appropriate recognition for the work of<lb/> their scientists. That said, individual scientists probably derive ben-<lb/>efits in their career trajectories from national reputations, though<lb/> more research needs to be done to confirm this.<lb/></p>

			<p>What remains to be seen is which factors do the most to dis-<lb/>tort citations from textual similarity. The visibility and quality of<lb/> research are both likely contributors, alongside funding levels and<lb/> overall reputations at the national level. However, as we do control<lb/> for the growth in the number of journals over time, this may be<lb/> one way to account for both the quality of published work and its<lb/> subsequent visibility. Similarly unsettled is the issue of whether the<lb/> distortion captured by the citational well could predict or stand in<lb/> as a proxy for any of these same factors <ref type="biblio">32-34</ref> .<lb/></p>

			<head>methods<lb/></head>

			<p>To capture citational lensing, we represent science as a multiplex network, L, with<lb/> three layers (Fig. <ref type="figure">1</ref>). Consider the simple case of citational lensing in a single field<lb/> in a given year t. L citation is the citation network between countries, where Lcitation i,j<lb/> contains the citation flow from country i to country j. To make things comparable<lb/> across the layers of the multiplex network, L citation is constructed as the number<lb/> of citations received by country i&apos;s papers published in the given field in year t by<lb/> all other countries j, where we use a five-year window after publication year t to<lb/> capture all citations from countries j from year t to year t + 5. In that way, the text<lb/> network based on published papers in year t corresponds to the citation network<lb/> of the number of cumulative citations received over the ensuing five years by<lb/> papers published in year t. We use z-scores for the edge weights rather than the<lb/> raw citation counts themselves.<lb/></p>

			<p>Another layer, L text , is a network where each connection Ltext i,j is the similarity<lb/> of the text of country i&apos;s research output to that of country j. To capture the degree<lb/> of similarity, we apply a unique supervised topic model called a labelled LDA<lb/> model <ref type="biblio">9</ref> . Using the nationalities of authors on papers, the NL-LDA model is unique<lb/> in that it captures the extent to which ideas and concepts embodied by n-grams in<lb/> the texts are associated to authors from which countries. This approach is useful<lb/> to disentangle and establish what is being studied in different countries, as many<lb/> papers are increasingly authored by researchers from different countries. The KLD 8<lb/> is taken for the similarity between countries in the text of their scientific papers. In<lb/> our case, the KLD measures how much information is lost going from the text of<lb/> one country i&apos;s scientific output to that of another country j.<lb/></p>

			<p>The reasoning here is similar to that used in other work in the science of<lb/> science <ref type="biblio">12</ref> . Information loss imitates the amount of work that scholars have to do<lb/> to communicate their ideas. When very little information is lost, communication<lb/> is seamless; when lots of information is lost, communication is difficult. Note<lb/> that this is not a symmetrical relationship, and that is by design. The L T<lb/> text layer<lb/> tends to identify the most common subject matter in national research, so<lb/> when information is lost in moving from country i to country j, it indicates that<lb/> researchers in country i publish about some topics that researchers in country j<lb/> do not (though this is, of course, usually a matter of degree rather than an issue<lb/> of presence and absence). This means that it is harder on average for a scientist in<lb/> country i to find a counterpart in country j that is working along a similar line of<lb/> research than it is for a scientist in country j to find a someone working on similar<lb/> problems in country i. This also means that it is easier to find a paper from country<lb/> j that cites a paper from country i than it is to find the reverse, assuming that<lb/> citations are more likely when two papers have the same subject matter.<lb/></p>

			<p>In principle, when the information loss is high going from i to j, we say that the<lb/> similarity of i to j is low. When very little information is lost going from i to j, we<lb/> say that the similarity of i to j is high. Just as in the citation layer, z-scores are used<lb/> for edge weights, the only difference being that we take the negative here in the text<lb/> layer, as high information loss implies exactly the opposite relationship that high<lb/> citations imply. So, when we compare the multiplex layer Lcitation i,j , which measures<lb/> citational flow from country i to country j, with Ltext i,j , which captures how similar<lb/> country i is to country j, we use the transpose of L text to result in L T<lb/> text , where the<lb/> similarity of country j to country i given as L T<lb/> texti,j is equivalent to Ltext j,i . L T<lb/> text is used<lb/> in equation <ref type="biblio">(1)</ref>. We use this transpose because the more researchers in country<lb/> j cite researchers in country i, we posit that the work produced by researchers in<lb/> country j (that is, who is doing the citing and thus is giving the attention to the<lb/> work being done in country i with their citations) ought to be more similar to the<lb/> work produced in country i (that is, who is being cited and receiving the attention<lb/> from country j). Distortions thus ought to reflect either over-recognition or<lb/> under-recognition via attention (vis-à-vis citations) relative to the work being<lb/> done elsewhere.<lb/></p>

			<p>The third layer, L distortion , is what we call the citational well (drawing on the idea<lb/> of gravity wells). This layer is constructed so that it will capture the difference<lb/> between the other two layers, as given by equation (1) and Fig. <ref type="figure">1</ref>. This means<lb/> that every L distortioni,j represents the distortion in the citation flow from country<lb/> i to country j, relative to what we would expect on the basis of the similarity of<lb/> the text written by country i&apos;s scientists to that written by scientists in country j.<lb/> Also implied is that the sum of the distortion for country j relative to every other<lb/> country in the network-country j&apos;s in-degree in L distortion -represents the total<lb/> distortion in the citation flow to country j.<lb/></p>

			<p>To illustrate how citational lensing can be applied, we use nearly 20 million<lb/> academic papers in nearly 150 fields and subfields from 1980 to 2012 in MAG, one<lb/> of the most extensive metadata repositories of academic publications.<lb/></p>

			<head>Articles<lb/> NATure HuMAN BeHAviOur<lb/></head>

			<p>These data include metadata such as citations, along with the abstract text of<lb/> published research articles. We show how citational lensing can be used to<lb/> characterize changes in the international scientific hierarchy over time and how it<lb/> can be scaled to cover all of science.<lb/></p>

			<p>MAG classifies journals into various fields, which provides a fairly reliable<lb/> reflection of disciplinary boundaries and allows for selection across a wide variety<lb/> of fields. MAG uses a six-tiered field classification ID scheme that is human<lb/> generated for the highest two levels. We primarily use the second-highest level,<lb/> which offers these more granular field divisions. So instead of using just &apos;physics&apos; ,<lb/> we consider &apos;astrophysics&apos; and &apos;nuclear physics&apos; to be their own fields because they<lb/> have different citation practices. Fields are identified and defined for our purposes<lb/> as their field IDs in MAG, and the fields are itemized in Supplementary Table <ref type="table">1</ref>.<lb/> We classify these fields into four broad categories: (1) biomedical, behavioural<lb/> and ecological sciences; (2) engineering and computational sciences; (3) physical<lb/> and mathematical sciences; and (4) social sciences. We use no other sort of field<lb/> normalization. The population of journals in MAG increases considerably over<lb/> time, which may partly affect the representation of countries in our analysis.<lb/></p>

			<p>L citation is assembled using the citation data in MAG. As mentioned above,<lb/> each Lcitation i,j holds the citation flow from country i to country j. Because citation<lb/> inflation <ref type="biblio">14</ref> distorts the volume of cumulative citations in a field over time,<lb/> rendering temporal comparisons biased, we standardize and &apos;deflate&apos; the number<lb/> of citations received in years t + n to the equivalent number of citations that would<lb/> have been received in the year t that the paper was published. In essence, the<lb/> citations received in a future t + n year are converted into an exchange rate based<lb/> on the year the paper was published t, rendering comparing citations across time<lb/> less biased by volume. (In Supplementary Figs. <ref type="figure">10-15</ref>, we rebuild our main figures<lb/> comparing the citation deflation method that we use here to two other conditions:<lb/> one that does not include any deflation and another that employs our own<lb/> deflation method focusing specifically on countries.)<lb/> L text is constructed using text from the abstracts and titles of each paper. This<lb/> has advantages over using the full texts of research papers, since some fields format<lb/> papers to emphasize methods over theory or vice versa, and others might have a<lb/> strict length criterion, in terms of word count or page length. Abstracts, however,<lb/> succinctly summarize the most important concepts in a paper. We restrict our<lb/> analysis to papers with English-only abstracts. (In Supplementary Figs. <ref type="figure">22-27</ref>,<lb/> we rebuild our main figures comparing these English-only abstracts to those that<lb/> were subsequently translated from their original language into English by us using<lb/> Google Translate.)<lb/></p>

			<p>We build both Lcitation i,j and L text using only those journals that have existed<lb/> in our data since 1980, the starting point of our analyses. (In Supplementary<lb/> Figs. <ref type="biblio">16</ref>-21, we rebuild our main figures including all journals irrespective of<lb/> their tenure in the data.) The important terms and phrases that represent ideas,<lb/> concepts and phenomena need to be efficiently extracted from abstract texts. So,<lb/> we construct each field&apos;s corpus in year t as a combination of unigrams, bigrams<lb/> and trigrams from every document&apos;s abstract, referred to as Field t . For our analyses<lb/> here, we use English-only abstracts to mitigate the risk of mistranslation. We also<lb/> translate non-English abstracts using a Python module called googletrans that<lb/> functions as an API with Google Translate and reconstruct our analyses, but our<lb/> conclusions are consistent with what we present here. We apply a phrase extraction<lb/> algorithm called RAKE (Rapid Automatic Keyword Extraction) to each abstract<lb/> to extract all important phrases and terms from unigrams through trigrams <ref type="biblio">35</ref> .<lb/> RAKE extracts terms and phrases from abstracts by analysing the frequency of<lb/> each n-gram and its co-occurrences with other n-grams in the text. An advantage<lb/> of RAKE over other approaches is that it is domain independent, so it does not rely<lb/> on a pretrained corpus to identify what terms are important. We then compiled an<lb/> &apos;academic stop word&apos; list of common phrases used in academic writing based on<lb/> Coxhead <ref type="biblio">36</ref> and removed them from the abstracts.<lb/></p>

			<p>KLD compares probability distributions. To process the text of scientific<lb/> articles so that each country has its own probability distribution, we apply<lb/> NL-LDA models on abstracts from MAG publication abstracts to measure how<lb/> similar or dissimilar the phenomena studied by researchers in different countries<lb/> are <ref type="biblio">2-4</ref> . We apply an NL-LDA model to each Field t corpus. This approach parses<lb/> the influence of countries on multi-authored, international papers, a staple of<lb/> many fields. We measure how similar individual countries&apos; unique national<lb/> signatures-or how strongly associated the terms found in a field&apos;s corpus in a year<lb/> are associated to researchers in some country x-are to one another. The NL-LDA<lb/> produces a matrix, φ Fieldt , where the rows are the n-grams in the corpus for Field t<lb/> defined as w m and the columns are the national signatures defined as C n . We<lb/> standardize each national signature (column) in φ Fieldt such that for each national<lb/> signature, we assign zero values to all terms that were not present in papers<lb/> authored from a particular country. (Our implementation of the NL-LDA model<lb/> assigns a very small non-zero value to all terms that are not present in documents<lb/> with a particular nation-label but are present in Field t .) As the national signatures<lb/> sum to 100%, we then renormalize each national signature after we convert the<lb/> associative probabilities of absent terms to zero so that the national signature still<lb/> sums to 100%.<lb/></p>

			<p>We first validate the quality of the nation-labels produced by the NL-LDA using<lb/> topic cohesion scores, the standard measure for how distinct a topic is from other<lb/> topics derived from the same model. A cohesive topic forms a distinctive grouping<lb/> of its top n-grams that differentiates it from other topics. However, to date, no<lb/> equivalent approach exists to measure nation-label cohesion for a supervised<lb/> model like the NL-LDA in the same way as topic cohesion does for unsupervised<lb/> models like the LDA. This is because the number of appropriate topics extracted<lb/> from an LDA is variable and somewhat subjective, but the NL-LDA nation-labels<lb/> are nominally fixed. That said, not every country may produce enough published<lb/> papers in a year to produce meaningful results, so including every country in<lb/> our analyses without any filtering may not be prudent. We apply the umass topic<lb/> cohesion measure to the nation-labels in each NL-LDA model, where we compare<lb/> the document co-occurrences of each nation-label&apos;s top 25 strongest associated<lb/> terms from its national signature. Whereas with unsupervised LDA models, lower<lb/> scores indicate more distinct and cohesive topics, with NL-LDA models, the<lb/> opposite holds true: nation-labels with strong national signatures lead the way in<lb/> global science and have lexical usage that is more widespread throughout the field.<lb/> For each NL-LDA model, we convert these scores into percentile ranks, where<lb/> the nation-labels that are the most ubiquitous (such as the United States and in<lb/> later years China) are in the highest percentile (that is, they have lower coherence<lb/> scores) and less active countries are in the lowest percentile (that is, they have<lb/> higher coherence scores). For the results presented here, all of the nation-labels<lb/> are included in the analyses. In Supplementary Figs. <ref type="figure">1-7</ref>, we rerun nearly all of the<lb/> figures presented here at the 25th and 75th percentiles. Our results broadly hold<lb/> despite the exclusion of nation-labels.<lb/></p>

			<p>With these matrices, we measure how similar any country&apos;s subject matter is<lb/> to that of all other countries for some Field t . However, a standard similarity score<lb/> (like a cosine similarity) is not directed, and our aim is to understand how much<lb/> one country looks like another when reciprocation may not happen. We compare<lb/> every country to every other country in φ Fieldt and take the KLD of every column<lb/> in φ Fieldt to every other column, where each comparison is a weighted, directed link<lb/> that creates an international network of asymmetric text similarity. To calculate<lb/> this score, we take the two vectors for a country i and another country j, presented<lb/> as their national signature vectors c i and c j , respectively, to determine how similar<lb/> they are to each other:<lb/></p>

				<formula>KLD(ci ∥ cj) =<lb/> ∑<lb/> ci log<lb/> ci<lb/> cj<lb/></formula>

				<label>(2)<lb/></label>

			<p>Here KLD measures how much information is lost by national signature c i<lb/> when approximated with the national signature from c j . In other words, the less<lb/> information that is lost by approximating c i with c j , the more similar c j is to c i .<lb/> From here, we construct 4,914 international networks of topic similarity across<lb/> nearly 150 academic fields and 33 years of data (that is, 1980 to 2012), defined as<lb/> KLD Fieldt (referred to in the results as L T<lb/> text ).<lb/> We create an upper bound for KLD in the following way: for each KLD<lb/> network, KLD Fieldt , we take the negative of its z-score, so that the lowest value<lb/> (that is, the lowest information loss and the most similar country dyad) is<lb/> normalized to be the largest value relative to all other edge weights in the<lb/> network (in terms of standard deviations). The dyad with the lowest raw<lb/> KLD score is thus the dyad where the least amount of information is lost by<lb/> approximating c i with c j , so that country i is highly aligned with country j.<lb/> This approach is advantageous as it renders comparison across networks<lb/> possible, particularly for extreme values.<lb/></p>

			<p>Statistics and reproducibility. Our analyses were observational, and no statistical<lb/> method was used to predetermine sample size.<lb/></p>

			<head>Reporting summary. Further information on research design is available in the<lb/></head>

			<figure>Nature Research Reporting Summary linked to this article.<lb/> 1<lb/> nature research | reporting summary<lb/> April 2020<lb/> Corresponding author(s): Charles J. Gomez<lb/> Last updated by author(s): Mar 3, 2022<lb/> Reporting Summary<lb/> Nature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency<lb/> in reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.<lb/></figure>

			<head>Statistics<lb/></head>

			<p>For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.<lb/></p>

			<head>n/a Confirmed<lb/></head>

			<p>The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement<lb/> A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly<lb/></p>

			<p>The statistical test(s) used AND whether they are one-or two-sided<lb/> Only common tests should be described solely by name; describe more complex techniques in the Methods section.<lb/></p>

			<head>A description of all covariates tested<lb/></head>

			<p>A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons<lb/> A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient)<lb/> AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)<lb/> For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted<lb/> Give P values as exact values whenever suitable.<lb/></p>

			<p>For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings<lb/> For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes<lb/> Estimates of effect sizes (e.g. Cohen&apos;s d, Pearson&apos;s r), indicating how they were calculated<lb/> Our web collection on statistics for biologists contains articles on many of the points above.<lb/></p>

			<head>Software and code<lb/></head>

			<p>Policy information about availability of computer code<lb/> Data collection Data are primarily from the publicly available Microsoft Academic Graph (MAG).<lb/></p>

			<head>Data analysis<lb/></head>

			<p>Data were analyzed, processed, and visualized using Python 3, R, and SQL. The data are stored on Athena (Amazon), queried and read in to<lb/> Python, analyzed with Python, and visualized with R.<lb/></p>

			<p>For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and<lb/> reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code &amp; software for further information.<lb/></p>

			<head>Data<lb/> Policy information about availability of data<lb/></head>

			<p>All manuscripts must include a data availability statement. This statement should provide the following information, where applicable:<lb/></p>

			<p>-Accession codes, unique identifiers, or web links for publicly available datasets<lb/> -A list of figures that have associated raw data<lb/> -A description of any restrictions on data availability<lb/> All data derived from the Microsoft Academic Graph (e.g., corpora, NL-LDA models, CSV edgelists, etc.), along with all relevant code in Python 3 and R used to<lb/> analyze and visualize these data, will be made available at the Harvard Dataverse upon publication.</p>


	</text>
</tei>
