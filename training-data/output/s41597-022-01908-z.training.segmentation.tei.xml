<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>1 <lb/>Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/>www.nature.com/scientificdata <lb/>A large dataset of scientific text <lb/>reuse in Open-access publications <lb/>Lukas Gienapp 1 ✉ , Wolfgang Kircheis 1,3 , Bjarne Sievers 1 , Benno Stein 2 &amp; Martin Potthast 1,3 ✉ <lb/>We present the Webis-STEREO-21 dataset, a massive collection of Scientific Text Reuse in Open-access <lb/>publications. It contains 91 million cases of reused text passages found in 4.2 million unique open-<lb/>access publications. Cases range from overlap of as few as eight words to near-duplicate publications <lb/>and include a variety of reuse types, ranging from boilerplate text to verbatim copying to quotations <lb/>and paraphrases. Featuring a high coverage of scientific disciplines and varieties of reuse, as well <lb/>as comprehensive metadata to contextualize each case, our dataset addresses the most salient <lb/>shortcomings of previous ones on scientific writing. The Webis-STEREO-21 does not indicate if a reuse <lb/>case is legitimate or not, as its focus is on the general study of text reuse in science, which is legitimate <lb/>in the vast majority of cases. It allows for tackling a wide range of research questions from different <lb/>scientific backgrounds, facilitating both qualitative and quantitative analysis of the phenomenon as <lb/>well as a first-time grounding on the base rate of text reuse in scientific publications. <lb/></front>

			<body>Background &amp; Summary <lb/>The reuse of text has a longstanding history in science. In qualitative research, besides verbatim quotations, the <lb/>techniques of paraphrasing, translation, and summarization are instrumental to both teaching and learning <lb/>scientific writing as well as to gaining new scientific insights 1 . In quantitative research, the use of templates as <lb/>an efficient way of reporting new results on otherwise standardized workflows is common 2 . As science often <lb/>progresses incrementally, authors may also reuse their texts across (different types of) subsequent publications <lb/>on the same subject (also called &quot;text recycling&quot;) 2-4 . Likewise, in interdisciplinary research, reuse across publica-<lb/>tions at venues of different disciplines has been observed to better promote the dissemination of new insights 5,6 . <lb/>Independent of all these manifestations of text reuse is the academic context that establishes their legitimacy: <lb/>In certain circumstances, any of them may be considered plagiarism, i.e., intentional reuse of a text without <lb/>acknowledging the original source, in violation of the honor code and academic integrity 7 . <lb/>Text reuse has been quantitatively studied in many scientific disciplines 1,2,8-10 ; yet few studies assess the phenom-<lb/>enon at scale beyond what can be manually analyzed 9,10 . Large-scale studies require the use of automatic text reuse <lb/>detection technology. This being both algorithmically challenging and computationally expensive, lack of expertise <lb/>or budget may have prevented such studies. Employing proprietary analysis software or services instead, too, is sub-<lb/>ject to budgetary limitations, in addition to their lack of methodological transparency and reproducibility. <lb/>Text reuse detection itself is still subject to ongoing research in natural language processing and information <lb/>retrieval. Setting up a custom processing pipeline thus demands an evaluation against the state of the art. The <lb/>challenges in constructing a competitive solution for this task arise from the aforementioned diversity of differ-<lb/>ent forms of text reuse, the large solution space of detection approaches, and the need to apply heuristics that <lb/>render a given solution sufficiently scalable. Preprocessing a collection of scientific publications, too, presents <lb/>its own difficulties. This includes the noisy and error-prone conversion of publications&apos; original PDF versions to <lb/>machine-readable texts and the collection of reliable metadata about the publications. The available quantitative <lb/>studies on scientific text reuse lack with respect to the presentation of preprocessing steps taken, the design <lb/>choices of the solution to text reuse detection, and their justification in terms of rigorous evaluation. Altogether, <lb/>comparable, reproducible, reliable, and accessible research on the phenomenon of scientific text reuse remains <lb/>an open problem. <lb/>To provide for a solid new foundation for the investigation of scientific text reuse within and across dis-<lb/>ciplines, we compile Webis-STEREO-21. To overcome the aforementioned issues, we stipulate three design <lb/></body>

			<front>1 Text Mining and Retrieval Group, Leipzig University, Leipzig, DE-04109, Germany. 2 Web Technology and <lb/>Information Systems Group, Bauhaus-Universität Weimar, Weimar, DE-99423, Germany. 3 ScaDS.AI, Center <lb/>for Scalable Data Analytics and Artificial Intelligence, Leipzig, DE-04105, Germany. ✉ e-mail: lukas.gienapp@ <lb/>uni-leipzig.de; martin.potthast@uni-leipzig.de <lb/>Data DeScriPtOr <lb/>OPeN <lb/></front>

			<page>2 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>principles for the creation of the dataset: (1) high coverage, both in terms of the number of included publications <lb/>and the variety of scientific disciplines; (2) a scalable approach to reuse detection with a focus on high preci-<lb/>sion at a competitive recall, capturing a comprehensive set of reused passages as reliable resource for research <lb/>on scientific text reuse; and (3) comprehensive metadata to contextualize each case, to address a wide range of <lb/>potential hypotheses. <lb/>Webis-STEREO-21 results from applying scalable text reuse detection approaches to a large collection of <lb/>scientific open-access publications, exhaustively comparing all documents to extract a comprehensive dataset <lb/>of reused passages between them. It contains more than 91 million cases of reused passages among 4.2 million <lb/>unique publications. The cases stem from 46 scientific fields of study, grouped into 14 scientific areas in all four <lb/>major scientific disciplines, and spanning over 150 years of scientific publishing between 1860 and 2018. The <lb/>data is openly accessible to be useful to a wide range of researchers with different scientific backgrounds, ena-<lb/>bling both qualitative and quantitative analysis. <lb/>Methods <lb/>To create a dataset of text reuse in scientific texts, four things are needed: an operationalization of the phenom-<lb/>enon of text reuse, a large collection of scientific publications, detailed metadata for each of these publications, <lb/>and a scalable yet effective method for detecting reuse in these publications. In this section we describe all four, <lb/>explaining the latter both generally and formally to ensure their reproducibility. Figure 1 illustrates the process-<lb/>ing pipeline for the creation of the dataset. <lb/>An operationalization of text reuse. To reuse something means to use it again after the first time. Reused <lb/>text is text that is primarily, if not exclusively, derived from another text. In academic writing, writing techniques <lb/>for reusing a text include boilerplate, quotation, paraphrasing, and summarizing. What all these techniques have <lb/>in common is that a reused text and its original have a certain kind of similarity that the reader can recognize 11 . <lb/>Manual identification of text reuse between two given texts is therefore based on the identification of the relevant <lb/>text passages where such similarities can be detected. <lb/>To operationalize this notion of reuse, a suitable text similarity measure and a similarity threshold have to be <lb/>chosen, where a pair of texts has to exceed the threshold in order to be considered potential reuse. We broadly <lb/>distinguish similarity measures that operate at the syntactic level from measures that operate at the semantic <lb/>level of language, where the former capture the &quot;reuse of words&quot; and the latter the &quot;reuse of ideas&quot; 12 . <lb/>In practice, text reuse detection relies heavily on syntactic similarity detection 13,14 . Syntactic reuse is relatively <lb/>easy to visualize and consequently faster to check than semantic reuse 15 . The former can be checked by distant <lb/>reading, the latter requires close reading, which causes high costs with increasing text length 16 . Similarly, it can <lb/>be assumed that semantic reuse is much less common than syntactic reuse, since the most common goal of text <lb/>reuse is to save time and cognitive effort, whereas the time savings of semantic reuse are generally lower 17 . <lb/>We opt for a conservative similarity analysis at the syntactic level, measuring the correspondence between <lb/>the surface forms of words occurring in two given texts and the phrases formed from them. This design decision <lb/>is also motivated by the target domain: In addition to the contribution of new ideas, a large fraction of scientific <lb/>contributions describe reflection on as well as advances to known ideas, and the development of solutions to <lb/>tasks and problems up to the point of transfer to practice. A semantic similarity score in this context would <lb/>rather lead to a citation graph mixing natural matches of ideas with intended reuse. It is in the nature of current <lb/>semantic similarity measures that they often capture even minor and distant similarities, leading to significant <lb/>noise in the form of false positives in tasks such as text reuse detection. Also, detection at the syntactic level <lb/>Webis STEREO-21 <lb/>91,466,374 Text Reuse Cases <lb/>Publication Database <lb/>4,656,302 PDF Files <lb/>Preprocessing <lb/>Text Alignment <lb/>Source Retrieval <lb/>Metadata <lb/>Fig. 1 Schematic overview of the text reuse detection pipeline. Each document is pre-processed and supplied <lb/>with metadata. Source retrieval identifies document pairs with local similarities, and alignment is applied to <lb/>identify reuse cases between those. <lb/></body>

			<page>3 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>is more in line with the concept of text reuse used in practice.In other words, &apos;similar&apos; here refers to an over-<lb/>lap in the vocabulary and wording of two documents, an operationalization common to large-scale text reuse <lb/>detection 18 . <lb/>Although even a single word overlap between two pieces of text, such as a specific and unlikely spelling mis-<lb/>take, can be sufficient as a strong indication of reuse, automatic methods are not yet able to reliably detect such <lb/>cases. Instead, several overlaps of words and phrases are required that occur in close proximity to each other <lb/>in both texts, thus forming potentially reused text passages. Such passages are not necessarily verbatim copies: <lb/>Words and phrases may be added, removed, or changed, and sentences may be rearranged. Nevertheless, suffi-<lb/>cient overlap must remain, which can be modeled by the similarity threshold parameter mentioned above. This <lb/>parameter cannot be derived formally, but must be determined empirically. <lb/>This operationalization of text reuse, and thus the cases included in our dataset, are orthogonal to the ques-<lb/>tion of the legitimacy of reuse. Thus, the notion of &quot;text reuse&quot; is explicitly not limited to or equated with &quot;plagia-<lb/>rism. &quot; We do not draw any conclusions from the fact that two documents have a reused passage in common, but <lb/>merely observe that there is one. The distinction between original and plagiarized text cannot and should not be <lb/>an algorithmic decision 19 . Further limitations are discussed in the Usage Notes section. <lb/>Text reuse detection in large document collections. Detecting all reuses of text in a collection of doc-<lb/>uments is a problem of high computational complexity. Each document must be compared pairwise with every <lb/>other document in the collection. So for N documents, N•(N-1) document-to-document comparisons must be <lb/>made. Each individual comparison requires a considerable amount of time and resources, so that the processing <lb/>of larger collections of documents exceeds even large computational capacities. In order to still be able to analyze <lb/>large document collections for text reuse, the set of comparisons is pruned by filtering out document pairs that <lb/>are guaranteed not to exhibit text reuse according to our operationalization. <lb/>This is achieved through a two-step process: first, a cheap-to-compute heuristic is applied to identify candi-<lb/>date pairs of documents where text reuse is likely. Then, the expensive document comparison is performed only <lb/>for the candidate pairs identified in this way. All other pairs are skipped.The first step of this two-part process <lb/>is commonly called a source retrieval 20 , while the second step is called text alignment 21 . The result of the whole <lb/>process is a set of cases of text reuse between documents in the collection. A case of text reuse is modeled here as <lb/>a pair of text passages, one in each document involved in the comparison, that have sufficient overlap of words <lb/>and phrases, along with references to their source documents and where exactly they are found in them. <lb/>While source retrieval greatly improves the efficiency of the overall process, it introduces an error in the form <lb/>of reduced recall, i.e., pairs of reused passages are overlooked that would have been discovered by an exhaustive <lb/>comparison of each pair of documents. The goal of source retrieval is therefore to filter out only those document <lb/>pairs for which the text alignment step will not find any reused passages, which is to be expected for the vast <lb/>majority of all document pairs. We choose the source retrieval parameters conservatively, so that only those <lb/>document pairs are skipped for which the text alignment is guaranteed to return no result. <lb/>Publication acquisition, preprocessing and metadata. Detecting scientific text reuse and contextual-<lb/>izing it on a large scale requires a large collection of scientific publications and detailed metadata about them. We <lb/>compiled such a dataset in the five steps of document selection, plain text extraction, text preprocessing, metadata <lb/>acquisition, and metadata standardization. <lb/>We build on the CORE dataset 22 , one of the largest collections of open-access scientific publications sourced <lb/>from more than 12,000 data providers. First, we identify the 6,015,512 unique open-access DOIs in the March <lb/>1, 2018 CORE dataset. Since the plain texts extracted from the PDF files of the publications, as provided by the <lb/>CORE data, are of varying quality and no structural annotations (such as markup for citations, in-text refer-<lb/>ences, section annotations) are available, we chose to obtain the original PDF files of the identified open-access <lb/>DOIs from various publicly available repositories. <lb/>The plain text extraction has been repeated on the acquired PDF files using the standardized state-of-the-art <lb/>toolchain GROBID 23 . A minimum of 1,000 and a maximum of 60,000 space-separated words are introduced <lb/>as an effective heuristic to filter out common plaintext extraction errors. In total, we obtained and extracted <lb/>clean plaintext for 4,267,166 documents (70% of the open access publications in the original CORE dataset). <lb/>Since text alignment analyzes word overlaps between documents, the highly standardized meta-information in <lb/>scientific texts such as citations, numbers, author names, affiliations, and references lead to exponentially more <lb/>false-positive detections of reuse than without them. Therefore, text alignment processes the abstract and main <lb/>body of a document without in-text references, tables, figures, bibliographic data, and numeric data, while nor-<lb/>malizing or removing special characters, which minimizes the false positives observed in preliminary studies. <lb/>However, research on text reuse and its evaluation on a case-by-case basis benefits from, or even requires, the <lb/>in-text metadata mentioned above, so that two versions of each text passage involved in a reuse case were kept: <lb/>the one that came from GROBID, including the aforementioned information except for tables and images, and, <lb/>the one that was fed into the text alignment. <lb/>Based on the detections, we augment the metadata provided by CORE with additional data from the <lb/>Microsoft Open Academic Graph (OAG) 24,25 , which contains study field annotations for a large number of publi-<lb/>cations. Metadata is assigned by matching records in CORE and OAG using an article&apos;s DOI identifier. Since the <lb/>annotated disciplines in the OAG do not follow a hierarchical scheme and since they are of different granularity <lb/>per publication (e.g., &quot;humanities&quot; as a whole vs. &quot;chemical solid state research&quot; as a subfield of chemistry), we <lb/>manually map the classification found in the OAG to the standard hierarchical DFG Classification of Scientific <lb/>Disciplines, Research Areas, Review Boards and Subject Areas 26 . We have chosen to replace the term &quot;review <lb/>board&quot; used in the DFG classification with the more conventional term &quot;field of study&quot;. The mapping was done <lb/>by three people independently. In the few cases where there was disagreement, consensus was reached through <lb/></body>

			<page>4 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>discussion. The final mapping includes 46 individual scientific fields of study drawn from 14 scientific areas and <lb/>four major scientific disciplines. <lb/>Source retrieval. An exhaustive comparison of all four million publications greatly exceeds the available <lb/>computing capacity. Therefore, a source retrieval step has to be carried out first 27,28 . By applying suitable heuris-<lb/>tics, the source retrieval step can be operationalized with linear time complexity with respect to the document <lb/>count 29 , compared to the quadratic expenditure of an exhaustive comparison. <lb/>The source retrieval component of our pipeline, i.e., the computation of all candidate comparisons D d for a <lb/>given document d, is operationalized by treating text reuse as a locally bounded phenomenon. Candidate pairs <lb/>are presumed to be identifiable by comparing text passages between two documents. If the similarity is suffi-<lb/>ciently high for at least one combination of passages between two documents d and d′, then d′ is considered a <lb/>candidate for d. <lb/>To achieve high scalability, we implement the similarity computation by applying locality-sensitive hash func-<lb/>tions h to each passage t in a document d, thus representing each document&apos;s passages with a set of hash values <lb/>h(t). These can be interpreted as a set of fingerprints, encoding the characteristics of each passage in a document <lb/>in compressed form. The similarity between two passages t and t′ stemming from two different documents d and <lb/>d′ is then approximated by the extent of overlap between their hash sets, h t <lb/>h t <lb/>( ) <lb/>( ) <lb/>∩ ′ . Thus, a document d′ is <lb/>considered a candidate source of text reuse for a document d, if at least one of their passage-level hash sets inter-<lb/>sects 29 : ∃ ⊆ <lb/>t d and t <lb/>d h t <lb/>h t <lb/>: ( ) <lb/>( ) <lb/>∩ <lb/>φ <lb/>∃ ′ ⊆ ′ <lb/>′ ≠ . This means that two documents have to share at least one of the <lb/>passage-level fingerprint hashes, i.e. have at least one common characteristic, to be deemed a candidate pair. <lb/>In our pipeline, the documents are first divided into consecutive passages of n terms, which in turn are each <lb/>embedded using a bag-of-words representation, i.e., as their term occurrence vector. To find matching passages <lb/>between two documents, MinHash 30 is chosen as hashing scheme for h. For each passage vector, multiple indi-<lb/>vidual hash functions are applied to each element, and the minimum hash value of each pass is saved, yielding <lb/>the set minimum hashes for a passage. It can be shown that, for m individual hashes per passage, two texts with <lb/>a Jaccard similarity of at least m -1 are guaranteed to produce a hash collision between their hash sets, rendering <lb/>them a suitable approximation for the lower bound of document similarity 30 . Applying this scheme to all pas-<lb/>sages in all documents, we detect all document pairs that are locally similar, i.e., which have a word overlap in <lb/>their bag-of-words passage representations. For example, if documents would be split into n = 20 word passages, <lb/>and m = 5 hash fingerprints are calculated per passage, two documents would be identified as pair if they share <lb/>at least 4 words across one of the respective passages. A document is never compared to itself. <lb/>Hash-based source retrieval allows for a significant reduction of the required computation time, since the <lb/>hash-based approximation has a linear time complexity with respect to |D|, as opposed to the quadratic com-<lb/>plexity of vector comparisons 29 . As a result, our source retrieval computation time could be fitted into the <lb/>allotted budget of two months of computing time on a 130-node Apache Spark cluster, with 12 CPU cores and <lb/>196 GB RAM per node (1560 cores, 250 TB RAM total). Besides its efficiency, the outlined approach provides <lb/>us with a second highly beneficial characteristic-the source retrieval step depends on only two parameters: the <lb/>passage length n, which can be chosen according to computational constraints, and the number of hashes m, <lb/>which is chosen according to the required minimum similarity separating two passages. By choosing this bound <lb/>of m -1 Jaccard similarity lower than the minimum detection threshold of the subsequent text alignment step, <lb/>the search space is pruned without a loss in accuracy. Thus, our hash-based similarity search primarily impacts <lb/>the precision, but not the recall of the source retrieval step with respect to the subsequent text alignment step. <lb/>Text alignment. Based on the reduced set of comparison candidates D d obtained by source retrieval, the text <lb/>alignment component extracts the exact location of the reused passages of each candidate pair of documents d <lb/>and d′ inD d . Here we follow the seed-and-extend approach to local sequence alignment 31 . The two-step process is <lb/>illustrated in Fig. 2. First, both texts are divided into small chunks (&apos;chunking&apos;). Then, the matching chunks in the <lb/>Cartesian product of the two sets of chunks are computed according to a similarity function ϕ. Its purpose is to <lb/>identify matching textual chunks (&apos;seeds&apos;&apos;) between two documents d and d′ that have the same meaning or can <lb/>be considered instances of the same concept. Finally, sufficiently close matches are combined into larger passages <lb/>(&apos;extension&apos;). Such a pair of passages ⊆ <lb/>′ ⊆ ′ <lb/>t d t <lb/>d <lb/>( <lb/>, <lb/>) in both documents is then output probable reuse case. <lb/>With respect to the chunking and seeding steps, two methodological decisions must be made. First, how to <lb/>divide a given document into small chunks. And second, which similarity function should be used to compare <lb/>d <lb/>d &apos; <lb/>d <lb/>d&apos; <lb/>d <lb/>d &apos; <lb/>Chunking <lb/>S eeding <lb/>E xtending <lb/>Fig. 2 Schematic overview of the text alignment process. Each document is divided into chunks, matching <lb/>chunks are identified between documents, and matches are extended to whole reuse cases. <lb/></body>

			<page>5 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>pairs of text chunks to determine whether they have the same meaning. A widely used approach to the former, <lb/>which has been shown to produce accurate results, is to divide a text into (word) n-grams, i.e., contiguous <lb/>chunks of words of length n 9,32 . These n-grams are overlapping and are created by &quot;sliding&quot; a window over the <lb/>text. For example, the sentence &quot;The quick brown fox jumps over the lazy dog. &quot; consists of the 4-grams &quot;The <lb/>quick brown fox&quot;, &quot;quick brown fox jumps&quot;, &quot;brown fox jumps over&quot;, &quot;fox jumps over the&quot;, &quot;jumps over the lazy&quot;, <lb/>and &quot;over the lazy dog&quot; at an overlap of three words. The n-gram chunks of the two documents d and d′ are then <lb/>exhaustively compared, i.e., every part of d is compared to every part of d′. As a similarity function, we again <lb/>use a hash function, but a string hash function rather than a locale-sensitive function. This choice enables a <lb/>linear-time comparison of the Cartesian product. Two parameters can be modified in this approach: the n-gram <lb/>size n, and the n-gram overlap k. <lb/>The matching chunks found through hash collisions indicate &quot;seeds&quot; of a potentially longer case of reused <lb/>text. To determine whether such a case can be found, the extension step joins co-aligned matching chunks <lb/>into longer passages when a sufficient number are found close to each other. In this manner, not only cases <lb/>of coherent reuse, but also cases where text was copied and then paraphrased can be detected. For example, if <lb/>consecutive sentences in a source text are copied into a target text, and then another (new) sentence is placed <lb/>in between them, this would be still considered a case of coherent reuse. Seeding would succeed in identifying <lb/>the two copied sentences on their own, yet the extension recognizes that both seeds are in close proximity in <lb/>both documents, and thus outputs a single case of reuse including both. The opposite case is also possible: two <lb/>chunks from different locations in a source text are placed close to each other in the target texts. Here, again, an <lb/>extension is required to reconstruct the full scope of reuse. Note that the extension approach depends on a single <lb/>parameter, Δ, which is the maximum distance in characters for two seeds in either of a pair of documents below <lb/>which a single reuse case is assumed 32 . <lb/>We supply a massively parallelizeable implementation of both the source retrieval and text alignment step, <lb/>which allows for detecting text reuse in the highly scalable manner needed given the amount and length of <lb/>input documents: 4.2 million publications, totaling 1.1 terabytes of text data. Overall, the text alignment step <lb/>accounted for an additional 3.5 months of computing time on the aforementioned Spark cluster. <lb/>Data records <lb/>Two types of data records are included in the Webis-STEREO-21 corpus: the reuse case data, which con-<lb/>tains all identified cases of text reuse alongside their metadata, and the publication data for each individual <lb/>document considered when computing the cases, including publication year and field of study annotations. The <lb/>records can be cross-referenced using a publications&apos; DOI as primary key. The corpus consists of two archive <lb/>files: cases.tar.gz contains the reuse case data, while publications.tar.gz contains the publication <lb/>data. Each of these archives bundles multiple partial files in the JSONL format, where every line corresponds to <lb/>a unique JSON-encoded case or publication. The Webis-STEREO-21 corpus is archived at Zenodo (https:// <lb/>doi.org/10.5281/zenodo.5575285 33 ). <lb/>Each of the 91,466,374 identified cases of potential text reuse is represented as an individual entry, refer-<lb/>encing two different publications. A pair of publications can contain multiple occurrences of text reuse, each of <lb/>which is treated as a unique case and entry. Each case encompasses two kinds of metadata: (1) locators, which <lb/>identify the matched text by its in-text location, using character offsets to mark start and end, and (2) context <lb/>about the publications involved in the case both by year and by field of study. In the context of a case, we refer <lb/>to the first involved publication as a, and to the second as b. This, however, does not indicate a directionality of <lb/>reuse in terms of publication time. Table 1 gives a detailed overview on all data fields available. <lb/>In the publication data, the metadata for all 4,267,166 documents considered for reuse computation is pro-<lb/>vided. Not every document is involved in at least one reuse case. Publications without detected reuse where <lb/>included to help contextualize analyses derived from the case data, providing baseline data about distribution <lb/>of metadata such as fields of study. Table 2 provides detailed information about the fields available for each <lb/>publication. <lb/>technical Validation <lb/>This section motivates and details the parameter choices for the source retrieval and text alignment components <lb/>of the text reuse detection pipeline. For both steps, we strive for maximum accuracy given the constraints for <lb/>scalability and computational efficiency imposed by the amount of data to be processed. To contextualize the <lb/>usability of the final dataset, key insights into the distribution of data are given as well. <lb/>Source retrieval. Objective of the source retrieval step is to prune the search space of document pairs by <lb/>reducing the number of pairs to be compared in subsequent (computationally expensive) steps. The optimization <lb/>criterion is recall: Ideally, no document pair containing text reuse should be overlooked, i.e., the false negative <lb/>rate should be minimized. <lb/>To allow for a fine-grained detection of local similarity, the parameters of the source retrieval step are set to <lb/>n = 50 and m = 10, i.e., documents are split into passages of 50 words, with 10 MinHash values computed per <lb/>passage. Consequently, the source retrieval is able to identify pairs of documents that share as little as a 9-word <lb/>overlap between any two passages. Note that these words do not need to be consecutive since passages are rep-<lb/>resented in a bag-of-words model. Since the subsequent alignment step operates at the 8-gram level (=eight <lb/>consecutive units, see next section), the filtering within the source retrieval step eliminates only pairs for which <lb/>the subsequent alignment step is guaranteed to not find any matches. Overall, the source retrieval step yields <lb/>3.305 × 10 12 total unique document pairs for further analysis. Given the initial document count of 4,656,302, this <lb/>represents a pruning of the search space by over 84% compared to an exhaustive pairwise comparison, rendering <lb/>the source retrieval step highly effective in improving overall efficiency. <lb/></body>

			<page>6 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>Text alignment. The text alignment step identifies the exact location and extent of reuse between two doc-<lb/>uments. Unlike source retrieval, text alignment is a precision-oriented task: Special emphasis is put on precision <lb/>over recall for the parameter choice to minimize the number of false positives; a low noise ratio is paramount <lb/>for meaningful future analyses of the dataset. To ensure the effectiveness of the text alignment, its parameters <lb/>are chosen by performing a grid-search, using precision, recall, and F 0.5 as effectiveness scores. We employ the <lb/>PAN-13 Text Alignment Corpus for evaluation, which has been previously published as a benchmark dataset for <lb/>text alignment by the PAN Shared Task on Plagiarism Detection 34 . It contains 10,000 pairs of documents that are <lb/>subject to different so-called obfuscation strategies that simulate more difficult cases of plagiarism, where the <lb/>authors tried to hide text-reuse by paraphrasing the copied text to some extent. <lb/>The applied obfuscation strategies include random obfuscation (shuffling, adding, deleting, and replac-<lb/>ing words or short phrases at random), cyclic translation obfuscation (a text is translated into another lan-<lb/>guage, and back to the original language; possibly with more languages in-between), summary obfuscation <lb/>(human-generated summaries of source texts), non-obfuscated plagiarism, and pairs of documents without <lb/>any text reuse between them. Each of these strategies is equally represented in the corpus with 2,000 pairs. The <lb/>grid search identifies an n-gram size of n = 8, an n-gram overlap of k = 7, and an extension range of Δ = 250 as <lb/>optimal. Under these conditions, our implementation of the text alignment step achieves a precision of 0.93 at <lb/>a recall of 0.46, and thus an F 0.5 score of 0.77. This renders our approach highly competitive when compared to <lb/>other approaches evaluated as part of the PAN Shared Tasks, placing it among the best for precision and F 0.5 , <lb/>while being the only approach adhering to the scalability requirements imposed by the scale of our analysis. <lb/>A full overview of the attained scores for comparison with competing systems is given in Table 3. <lb/>When evaluating separately per obfuscation strategy (Table 4), precision is very high throughout, exceeding <lb/>0.88 in all cases. This fact places our system within 0.08 for the best-performing, yet computationally more <lb/>complex approach for each obfuscation strategy. Yet, a drop in recall can be observed for heavily obfuscated text, <lb/>down to 0.1 for summary obfuscation. This effect is expected and also noticeable among the other approaches <lb/>presented at PAN. Moreover, our focus is not on plagiarism but on establishing a general baseline for text reuse <lb/>in science, where more literal reuse (e.g., citations, idiomatic reuse, template writing, etc.) is expected to be the <lb/>norm. The heavily obfuscated test sets studied at PAN were dedicated to study extreme cases of plagiarism, <lb/>Key <lb/>Description <lb/>Type <lb/>id <lb/>Unique identifier for this case, in UUID format. <lb/>String <lb/>Publication A <lb/>Locator <lb/>begin_a <lb/>Start location of matched text, measured as character offset. Integer <lb/>end_a <lb/>End location of matched text, measured as character offset. Integer <lb/>doc_length_a <lb/>Total length of publication A in characters. <lb/>Integer <lb/>Context <lb/>doi_a <lb/>DOI identifier for publication A. <lb/>String <lb/>year_a <lb/>Publication year for publication A. <lb/>Integer (optional) <lb/>field_a <lb/>Field(s) of study for publication A. <lb/>String Array (optional) <lb/>area_a <lb/>Scientific area(s) for publication A. <lb/>String Array (optional) <lb/>discipline_a <lb/>Scientific discipline(s) for publication A. <lb/>String Array (optional) <lb/>Publication B <lb/>Locator <lb/>begin_b <lb/>Start location of matched text, measured as character offset. Integer <lb/>end_b <lb/>End location of matched text, measured as character offset. Integer <lb/>doc_length_b <lb/>Total length of publication B in characters. <lb/>Integer <lb/>Context <lb/>doi_b <lb/>DOI identifier for publication B. <lb/>String <lb/>year_b <lb/>Publication year for publication B. <lb/>Integer (optional) <lb/>field_b <lb/>Field(s) of study for publication B. <lb/>String Array (optional) <lb/>area_b <lb/>Scientific area(s) for publication B. <lb/>String Array (optional) <lb/>discipline_b <lb/>Scientific discipline(s) for publication B. <lb/>String Array (optional) <lb/>Table 1. Data record overview for case data. Key denotes the top-level JSON key by which fields are identified, <lb/>Description provides an explanation of the contained data, and Type denotes the data type; &quot;optional&quot; indicates <lb/>fields that can be empty. The text fields are included only in the full version of the corpus. <lb/>Key <lb/>Description <lb/>Type <lb/>Metadata <lb/>doi <lb/>DOI identifier of the publication. <lb/>String <lb/>doc_length <lb/>Total length of the publication in characters. Integer <lb/>year <lb/>Publication year of the publication. <lb/>String Array (optional) <lb/>field <lb/>Field of study of the publication. <lb/>String Array (optional) <lb/>area <lb/>Scientific area of the publication. <lb/>String Array (optional) <lb/>discipline <lb/>Scientific discipline of the publication. <lb/>String Array (optional) <lb/>Table 2. Data record overview for publication data. Key denotes the top-level JSON key fields are identified by, <lb/>Description provides an explanation of the contained data, and Type denotes the data type; &quot;optional&quot; indicates <lb/>fields that can be empty. <lb/></body>

			<page>7 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>where an author expends much effort to hide the fact via severe forms of paraphrasing. The investment of such <lb/>an effort, however, goes against the time savings that can be expected from reusing text, so that it can be pre-<lb/>sumed that the vast amount of paraphrased text reuse will only make smaller changes to a text instead of chang-<lb/>ing every last n-gram. This observation is partially corroborated when reviewing the many cases of academic <lb/>plagiarism found in dissertation theses throughout the recent years 14 , where extensive paraphrasing is hardly <lb/>ever observed. We therefore deemed the investment of an exponentially higher computation cost into retrieving <lb/>such cases to be uneconomical. <lb/>Nevertheless, two measures to tackle this recall issue have been proposed by PAN participants: (1) custom-<lb/>ized approaches for each of the different obfuscation strategies, employing heuristics to detect which kind a doc-<lb/>ument pair is exhibiting, and (2) ensemble methods encompassing different seeding and extension strategies, <lb/>combined into a single result. However, the first is not applicable to our situation, as there is no ground truth <lb/>data available to fine-tune such a classification to scientific writing. The second comes at a very high runtime <lb/>and algorithmic complexity. This is reflected in Table 3 as well: for each approach, the (asymptotic) algorithmic <lb/>complexity is noted, for two given sequences of length n and m. Only six of the approaches presented at PAN <lb/>perform in sub-quadratic time, a necessary requirement for large-scale detection. Furthermore, the data spec-<lb/>ificity for each approach is listed: it denotes how much fine-tuning to the PAN data has taken place, for exam-<lb/>ple, by crafting specialized corpus-dependent features, using ensemble methods, trained classifiers, or specific <lb/>approaches for each of the different obfuscation strategies, all of which reduce the generalizability and transfer-<lb/>ability of the approaches to other data. <lb/>Out of the four other approaches to combine sub-quadratic runtime complexity with low data specificity, <lb/>ours performs best with regard to recall and the F 0.5 score, and is second in precision by a very close margin. <lb/>PAN13 Evaluation Corpus <lb/>Team <lb/>Precision ↓ <lb/>Recall <lb/>F0.5 <lb/>Complexity <lb/>Data Specificity <lb/>Glinos (2014) <lb/>0.96 <lb/>0.79 <lb/>0.92 O(nm) <lb/>Medium <lb/>Jayapal (2012) <lb/>0.95 <lb/>0.22 <lb/>0.57 O(n + m) <lb/>Low <lb/>Nourian (2013) <lb/>0.95 <lb/>0.43 <lb/>0.76 -<lb/>-<lb/>Gross (2014) <lb/>0.93 <lb/>0.77 <lb/>0.89 O(nm) <lb/>Low <lb/>Alvi (2014) <lb/>0.93 <lb/>0.55 <lb/>0.82 O(n + m) <lb/>Medium <lb/>Ours <lb/>0.93 <lb/>0.46 <lb/>0.77 O(n + m) <lb/>Low <lb/>Baseline (2014) <lb/>0.93 <lb/>0.34 <lb/>0.69 O(n + m) <lb/>Low <lb/>Palkovskii (2014) <lb/>0.92 <lb/>0.83 <lb/>0.90 O(nm) <lb/>High <lb/>Torrejon (2014) <lb/>0.90 <lb/>0.77 <lb/>0.87 O(nm) <lb/>High <lb/>Gillam (2014) <lb/>0.89 <lb/>0.17 <lb/>0.48 O(nm) <lb/>High <lb/>Oberreuter (2014) <lb/>0.89 <lb/>0.86 <lb/>0.88 -<lb/>-<lb/>Oberreuter (2012) <lb/>0.89 <lb/>0.77 <lb/>0.86 -<lb/>-<lb/>Gillam (2012) <lb/>0.89 <lb/>0.27 <lb/>0.61 O(nm) <lb/>High <lb/>Torrejon (2013) <lb/>0.89 <lb/>0.76 <lb/>0.86 O(nm) <lb/>High <lb/>Gillam (2013) <lb/>0.88 <lb/>0.26 <lb/>0.60 O(nm) <lb/>High <lb/>Jayapal (2013) <lb/>0.88 <lb/>0.38 <lb/>0.70 O(n + m) <lb/>Low <lb/>Sanchez-Perez (2014) <lb/>0.88 <lb/>0.88 <lb/>0.88 O(nm) <lb/>High <lb/>Kueppers (2012) <lb/>0.87 <lb/>0.51 <lb/>0.76 O(nm) <lb/>Low <lb/>Shrestha (2013) <lb/>0.87 <lb/>0.74 <lb/>0.84 O(nm) <lb/>Medium <lb/>Saremi (2013) <lb/>0.87 <lb/>0.77 <lb/>0.85 -<lb/>-<lb/>Shrestha (2014) <lb/>0.86 <lb/>0.84 <lb/>0.86 O(nm) <lb/>Medium <lb/>Kong (2012) <lb/>0.85 <lb/>0.82 <lb/>0.84 O(nm) <lb/>Low <lb/>Suchomel (2012) <lb/>0.84 <lb/>0.65 <lb/>0.79 O(n + m) <lb/>Medium <lb/>Kong (2014) <lb/>0.84 <lb/>0.81 <lb/>0.83 O(nm) <lb/>Low <lb/>Kong (2013) <lb/>0.83 <lb/>0.81 <lb/>0.83 O(nm) <lb/>Low <lb/>Torrejon (2012) <lb/>0.83 <lb/>0.75 <lb/>0.81 O(nm) <lb/>High <lb/>Palkovskii (2013) <lb/>0.82 <lb/>0.54 <lb/>0.74 O(nm) <lb/>High <lb/>Palkovskii (2012) <lb/>0.82 <lb/>0.76 <lb/>0.81 O(nm) <lb/>High <lb/>Abnar (2014) <lb/>0.77 <lb/>0.61 <lb/>0.73 O(nm) <lb/>Medium <lb/>Suchomel (2013) <lb/>0.73 <lb/>0.77 <lb/>0.74 O(n + m) <lb/>Medium <lb/>Sanchez-Vega (2012) <lb/>0.40 <lb/>0.56 <lb/>0.42 O(nm) <lb/>Medium <lb/>Table 3. Precision, Recall, and F 0.5 score of competing alignment approaches by teams participating at the PAN <lb/>Shared Tasks, taken from Potthast et al. 34 and Potthast et al. 39 and sorted descending by precision. The listed <lb/>approaches are described in detail in the PAN Workshop proceedings 40-42 . Complexity denotes the runtime <lb/>complexity of the approach; Data Specificity denotes the degree of PAN-specific optimizations reducing <lb/>transferability to other data domains, i.e. ensemble methods, trained approaches, feature extraction, or special <lb/>handling of corpus characteristics. <lb/></body>

			<page>8 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>Against this background, our achieved detection performance is comparably outstanding, given the specialized <lb/>requirements in terms of scalability as well as the focus on a high-precision identification. <lb/>Dataset properties. Table 5 lists the distribution of identified cases and original publications across scientific <lb/>disciplines. The relative share of disciplines is approximately the same for both, with only natural sciences having a <lb/>decreased share in cases compared to publications. Figure 3 shows cumulative ratios of cases per normalized length, <lb/>i.e., matched case length divided by total publication length, and normalized position, i.e., start offset divided by total <lb/>publication length. Less than one percent of cases encompasses more than 20% of the original publication. 90% of <lb/>cases cover at most 1% of the original publication. Some duplicate publications were contained in the dataset under <lb/>different DOIs, as made evident by the small spike at normalized length larger than 0.99. Most of the reuse cases occur <lb/>in the last 5% of a publication. This is likely due to author contribution statements, copyright notices, or acknowl-<lb/>edgements contributing a majority of boilerplate text reuse. The distribution over the rest of the relative positions is <lb/>approximately uniform. Overall, these three key properties highlight the validity of the data, and, combined with the <lb/>overall amount of cases, this allows to build focused subsets of substantial size for downstream tasks and analysis. <lb/>Usage Notes <lb/>Applications and examples. Based on the various reuse cases contained in Webis-STEREO-21, a <lb/>variety of research questions can be answered. These include the study of discipline-specific writing practices, <lb/>comparative studies, and a variety of machine learning tasks. The corpus includes a wide range of reuse cases, <lb/>and the operationalization of text reuse is deliberately chosen to encompass many phenomena of reuse. Webis-<lb/>STEREO-21 contains mainly &quot;innocent&quot; instances of reuse, such as short phrases reused by the same authors, <lb/>standardized technical language, wording prescribed by publishers (e.g., licenses), or established wording of <lb/>recurring blocks of text (e.g., author contributions). In particular, the corpus is not a collection of plagiarism <lb/>cases, and we refrain from judging the legitimacy of the cases. <lb/>PAN13 Evaluation Corpus <lb/>Precision <lb/>Recall <lb/>F0.5 <lb/>No Obfuscation <lb/>0.88 <lb/>0.90 <lb/>0.88 <lb/>No Plagiarism <lb/>1.00 <lb/>1.00 <lb/>1.00 <lb/>Random Obfuscation <lb/>0.90 <lb/>0.11 <lb/>0.37 <lb/>Summary Obfuscation <lb/>0.99 <lb/>0.10 <lb/>0.36 <lb/>Translation Obfuscation <lb/>0.88 <lb/>0.16 <lb/>0.46 <lb/>Entire Corpus <lb/>0.93 <lb/>0.46 <lb/>0.77 <lb/>Table 4. Precision, Recall, and F 0.5 of the text alignment component per obfuscation strategy and on the <lb/>complete evaluation corpus. <lb/>Discipline <lb/>Cases <lb/>Publications <lb/>Natural Sciences <lb/>21,504,070 <lb/>(24%) 1,606,599 (38%) <lb/>Engineering Sciences <lb/>14,753,613 <lb/>(16%) 911,226 <lb/>(21%) <lb/>Life Sciences <lb/>43,358,077 <lb/>(47%) 1,646,843 (39%) <lb/>Humanities &amp; Social Sciences 15,265,777 <lb/>(17%) 748,298 <lb/>(18%) <lb/>Total <lb/>91,466,374 <lb/>4,267,166 <lb/>Table 5. Number and ratio of cases and publications per scientific discipline. Percentages can exceed 100 in <lb/>sum due to multiple membership. <lb/>0.0 <lb/>0 .2 <lb/>0.4 <lb/>0 .6 <lb/>0.8 <lb/>1 .0 <lb/>Normalized Position <lb/>10 2 <lb/>10 1 <lb/>10 0 <lb/>Cumulative Ratio <lb/>(b) <lb/>0.0 <lb/>0 .2 <lb/>0.4 <lb/>0 .6 <lb/>0.8 <lb/>1 .0 <lb/>Normalized Length <lb/>10 2 <lb/>10 1 <lb/>10 0 <lb/>Cumulative Ratio <lb/>(a) <lb/>Fig. 3 (a) cumulative ratio of reuse cases by normalized case length. (b) cumulative ratio of reuse cases by <lb/>normalized position. <lb/></body>

			<page>9 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<body>To illustrate the variety of reuse cases, Table 6 shows three examples, each representing a different type of <lb/>legitimate reuse. The first is a quotation, where two authors each quote verbatim from a tertiary source. The sec-<lb/>ond is an example of a paraphrase and comes from two different articles by the same author on the same topic. <lb/>The third is an example of reuse of boilerplate text, as both are contribution statements whose structure is highly <lb/>formalized, the difference being the different contributions of the various authors. Because of the many types of <lb/>reuse, answering a particular research question may require filtering techniques to create a subset suitable for the <lb/>application at hand. The metadata included allows for fine-grained filtering, e.g., by location in the text, extent, <lb/>time, and specific scientific fields. <lb/>Data accessibility. The dataset is distributed as JSONL files of reuse case metadata to enable efficient stream-<lb/>ing and filtering even with low computational resources. Since some analyses require the text of the reuse cases <lb/>Webis-STEREO-21, researchers can hydrate the corpus using the text preprocessing component of our pro-<lb/>cessing pipeline, which is included with the dataset as a standalone Python script. This enables the conversion of <lb/>GROBID extraction results, e.g. from the CORE repository, into a compatible text format so that the locators con-<lb/>tained in the dataset can be used to recover the text portions of individual reuse cases. Since the publications the <lb/>corpus is based upon are open access, the original PDF files can be easily retrieved by their DOI, further lowering <lb/>the barrier of access. We also provide the code used to compute the corpus statistics in this article to give others <lb/>interested in working with the data an example of use. <lb/>ethical considerations. Our dataset includes contemporary scientific texts (&quot;papers&quot;) with the goal of <lb/>examining the occurrence, nature, and types of text reuse that result from scientific writing practices. Given <lb/>general ethical considerations for datasets 35 , three are particularly relevant to the proposed collection: (1) privacy <lb/>of the individuals included in the data, (2) effects of biases on downstream use, and (3) dataset usage for dubious <lb/>purposes. We therefore took into account a consensus on best-practices for ethical dataset creation 36-38 . <lb/>Ad (1). While the corpus does not contain author names or identifiers, all included papers are freely accessi-<lb/>ble, so it is possible to determine the identity of individual authors by referring to the original CORE/OAG data <lb/>or simply by accessing a DOI. However, we do not consider this to be problematic, since all those who participate <lb/>professionally in scientific discourse agree, by publishing their contributions, that they will go down in the scien-<lb/>tific annals under their name and that they can be examined by anyone. This is especially true for articles under <lb/>an open access license, where consent to create derivative works, public archiving, and exploitation is implied. <lb/>Ad (2). Two types of bias can arise from our process of dataset curation. First, by using only open access pub-<lb/>lications, the types, disciplines, and characteristics of the papers included may not be representative of science <lb/>as a whole. Since it is not yet possible to analyze all scientific papers ever published, we try to minimize this risk <lb/>by using as large a sample as possible. Second, the operationalization and implementation of text reuse may vary <lb/>for more specific research questions on this topic. Our goal was therefore to employ a very general and inclusive <lb/>detection approach. For downstream tasks, some of the cases included are not interesting, and the data can fil-<lb/>tered to obtain a more targeted collection. Moreover, by reproducibly documenting its creation process, we aim <lb/>to maximize its extensibility in the future. <lb/>Ad (3). We estimate the potential for misuse of the dataset to be low. One contentious issue is the use of the <lb/>dataset to broadly target academics on their writing practices. However, this has not been done with previously <lb/>published text reuse datasets. The novelty of this dataset compared to previous datasets is the scope, from which <lb/>no new doubtful use cases emerge. Furthermore, the dataset explicitly refrains from classifying the legitimacy of <lb/>reuse cases. Both the operationalization of the term and its intended use are intended to examine all types and <lb/>techniques of reuse of text in science, not plagiarism in particular. <lb/>Publication A <lb/>Publication B <lb/>[…] children than men in lower classes nor experienced the same downward <lb/>mobility. Clark writes, &quot;Thus we may speculate that England&apos;s advantage lay <lb/>in the rapid cultural, and potentially also genetic, diffusion of the values of <lb/>the economically successful throughout society in the years 1200-1800&quot; <lb/>(p. 271). It does not serve the book well to dwell on the rebuttal of its evolutionary <lb/>[…] https://doi.org/10.1111/j.1475-4991.2009.00354.x <lb/>[…] Impact on the standard of living, but eventually they led to the end of the <lb/>long Malthusian era. &quot;Thus we may speculate that England&apos;s advantage lay in <lb/>the rapid cultural, and potentially also genetic, diffusion of the values of the <lb/>economically successful throughout society in the years 1200-1800&quot; (p. 271). <lb/>Finally, Clark considers the great divergence among today&apos;s economies. Because <lb/>[…] https://doi.org/10.1111/j.1468-0289.2008.00432_10.x <lb/>[…] static approximation of the ColourSinglet Model, we have seen that the <lb/>production amplitude receives contributions from two different cuts. The <lb/>first one in its static limit gives the colour-singlet mechanism. The second <lb/>one has not been considered so far. We treat it in a gauge-invariant manner <lb/>by introducing necessary new 4-point vertices, suggestive of the colour-<lb/>octet mechanism. This new contribution can be as large as the colour-singlet <lb/>mechanism at high https://doi.org/10.1063/1.2122163 <lb/>[…] and we show that the lowest-order mechanism for heavy-quarkonium <lb/>production receives in general contributions from two different cuts. The <lb/>first one corresponds to the usual colour-singlet mechanism. The second one <lb/>has not been considered so far. We treat it in a gauge-invariant manner, and <lb/>introduce new 4-point vertices, suggestive of the colour-octet mechanism. <lb/>These new objects enable us to go beyond the static approximation. We show that <lb/>the contribution of […] https://doi.org/10.1016/j.physletb.2005.11.073 <lb/>[…] The authors declare that they have no competing interests. MS and MMD <lb/>drafted and wrote the manuscript. MS, MMD and KCRP participated in the <lb/>care of the patient and interpretation of the investigations. All authors read and <lb/>approved the final manuscript. […] https://doi.org/10.1186/1757-1626-2-147 <lb/>[…] The authors declare that they have no competing interests. Authors&apos; <lb/>contributions: AJB is the chief investigator of the CASP trial, he is responsible <lb/>for the conduct of the study and the clinical supervision and training of the <lb/>therapists and he wrote the first draft of this paper. CS drafted an earlier version <lb/>of the grant proposal and collaborated with AJB, MT, RR &amp; PH to produce <lb/>the successful proposal. CS also provided clinical supervision and therapist <lb/>training. LS edited and revised the proposal and is responsible for the day to <lb/>day running of the trial. All authors read and approved the final manuscript. <lb/>[…] https://doi.org/10.1186/1471-244X-13-199 <lb/>Table 6. Matched text with before and after context for selected reuse examples. Portions with highest similarity <lb/>in bold. DOIs of original publications given. <lb/></body>

			<page>10 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<div type="availability">code availability <lb/>The complete source code used for candidate retrieval and text alignment is openly accessible and permanently <lb/>available on GitHub (https://github.com/webis-de/scidata22-stereo-scientific-text-reuse). The data processing <lb/>pipeline is written in Python 3.7, utilizing the pyspark framework. The compute cluster on which we carried out <lb/>the data processing and our experiments run Spark Version 2.4.8. The text alignment component is written in <lb/>Go 1.16 and can be used as a standalone application. Detailed documentation about each pipeline component, <lb/>recommendations for compute resources, and suggestions for parameter choices are distributed alongside the <lb/>code to facilitate code reuse. <lb/></div>

			<front>Received: 8 September 2022; Accepted: 14 December 2022; <lb/>Published: xx xx xxxx <lb/>references <lb/></front>

			<listBibl>1. Sun, Y.-C. &amp; Yang, F.-Y. Uncovering published authors&apos; text-borrowing practices: Paraphrasing strategies, sources, and self-<lb/>plagiarism. Journal of English for Academic Purposes 20, 224-236, https://doi.org/10.1016/j.jeap.2015.05.003 (2015). <lb/>2. Anson, I. G. &amp; Moskovitz, C. Text recycling in stem: a text-analytic study of recently published research articles. Accountability in <lb/>Research 28, 349-371 (2020). <lb/>3. Moskovitz, C. Self-plagiarism, text recycling, and science education. BioScience 66, 5-6, https://doi.org/10.1093/biosci/biv160 (2015). <lb/>4. Hall, S., Moskovitz, C. &amp; Pemberton, M. A. Attitudes toward text recycling in academic writing across disciplines. Accountability in <lb/>Research 25, 142-169, https://doi.org/10.1080/08989621.2018.1434622 (2018). <lb/>5. Bird, S. J. Self-plagiarism and dual and redundant publications: what is the problem? Science and engineering ethics 8, 543-544 (2002). <lb/>6. Wen, Q. &amp; Gao, Y. Dual publication and academic inequality. International Journal of Applied Linguistics 17, 221-225 (2007). <lb/>7. Eberle, M. E. Paraphrasing, plagiarism, and misrepresentation in scientific writing. Transactions of the Kansas Academy of Science <lb/>(1903-) 116, 157-167 (2013). <lb/>8. Ganascia, J.-G., Glaudes, P. &amp; Del Lungo, A. Automatic detection of reuses and citations in literary texts. Literary and Linguistic <lb/>Computing 29, 412-421, https://doi.org/10.1093/llc/fqu020 (2014). <lb/>9. Citron, D. T. &amp; Ginsparg, P. Patterns of text reuse in a scientific corpus. Proceedings of the National Academy of Sciences 112, 25-30, <lb/>https://doi.org/10.1073/pnas.1415135111 (2015). <lb/>10. Horbach, S. S. &amp; Halffman, W. W. The extent and causes of academic text recycling or &apos;self-plagiarism&apos; . Research Policy 48, 492-502 (2019). <lb/>11. Foltýnek, T. et al. Testing of support tools for plagiarism detection. International Journal of Educational Technology in Higher <lb/>Education 17, 1-31 (2020). <lb/>12. Sadeghi, R. The attitude of scholars has not changed towards plagiarism since the medieval period: definition of plagiarism <lb/>according to shams-e-qays, thirteenth-century persian literary scientist. Research Ethics 15, 1-3, https://doi. <lb/>org/10.1177/1747016116654065 (2019). <lb/>13. Moskovitz, C. Standardizing terminology for text recycling in research writing. Learned Publishing 34 (2021). <lb/>14. Various Authors. Vroniplag Wiki. https://vroniplag.fandom.com/. Accessed: 2021-12-14 (2021). <lb/>15. Riehmann, P., Potthast, M., Stein, B. &amp; Fröhlich, B. Visual assessment of alleged plagiarism cases. Computer Graphics Forum 34, <lb/>1-10, https://doi.org/10.1111/cgf.12618 (2015). <lb/>16. Moretti, F. Distant reading (Verso Books, 2013). <lb/>17. Potthast, M., Hagen, M., Völske, M. &amp; Stein, B. Crowdsourcing interaction logs to understand text reuse from the web. In Fung, P. &amp; <lb/>Poesio, M. (eds.) 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), 1212-1221 (Association for <lb/>Computational Linguistics, 2013). <lb/>18. Martin, B. Plagiarism: policy against cheating or policy for learning. Nexus (Newsletter of the Australian Sociological Association) 16, <lb/>15-16 (2015). <lb/>19. Weber-Wulff, D. Plagiarism detectors are a crutch, and a problem. Nature 567, 435 (2019). <lb/>20. Stein, B., Meyer zu Eißen, S. &amp; Potthast, M. Strategies for retrieving plagiarized documents. In Clarke, C., Fuhr, N., Kando, N., <lb/>Kraaij, W. &amp; de Vries, A. (eds.) 30th International ACM Conference on Research and Development in Information Retrieval (SIGIR <lb/>2007), 825-826, https://doi.org/10.1145/1277741.1277928 (ACM, New York, 2007). <lb/>21. Potthast, M. et al. Overview of the 5th international competition on plagiarism detection. In Forner, P., Navigli, R. &amp; Tufis, D. (eds.) <lb/>Working Notes Papers of the CLEF 2013 Evaluation Labs, vol. 1179 of Lecture Notes in Computer Science (2013). <lb/>22. Knoth, P. &amp; Zdráhal, Z. CORE: three access levels to underpin open access. D Lib Mag. 18, https://doi.org/10.1045/november2012-<lb/>knoth (2012). <lb/>23. Lopez, P. &amp; Romary, L. GROBID -information extraction from scientific publications. ERCIM News 2015 (2015). <lb/>24. Tang, J. et al. Arnetminer: extraction and mining of academic social networks. In Li, Y., Liu, B. &amp; Sarawagi, S. (eds.) Proceedings of <lb/>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, <lb/>2008, 990-998, https://doi.org/10.1145/1401890.1402008 (ACM, 2008). <lb/>25. Sinha, A. et al. An overview of microsoft academic service (MAS) and applications. In Gangemi, A., Leonardi, S. &amp; Panconesi, A. <lb/>(eds.) Proceedings of the 24th International Conference on World Wide Web Companion, WWW 2015, Florence, Italy, May 18-22, <lb/>2015 -Companion Volume, 243-246, https://doi.org/10.1145/2740908.2742839 (ACM, 2015). <lb/>26. Deutsche Forschungsgemeinschaft. DFG classification of scientific disciplines, research areas, review boards and subject areas. <lb/>https://web.archive.org/web/20201126170513/https://www.dfg.de/download/pdf/dfg_im_profil/gremien/fachkollegien/ <lb/>amtsperiode_2016_2019/fachsystematik_2016-2019_en_grafik.pdf Accessed on 2021-05-27 (2016). <lb/>27. Hagen, M., Potthast, M. &amp; Stein, B. Source retrieval for plagiarism detection from large web corpora: recent approaches. In <lb/>Cappellato, L., Ferro, N., Jones, G. &amp; San Juan, E. (eds.) Working Notes Papers of the CLEF 2015 Evaluation Labs, vol. 1391 of Lecture <lb/>Notes in Computer Science (2015). <lb/>28. Hagen, M., et al. (eds.) 26th ACM International Conference on Information and Knowledge Management (CIKM 2017), 2091-2094, <lb/>https://doi.org/10.1145/3132847.3133097 (ACM, 2017). <lb/>29. Alshomary, M. et al. Wikipedia text reuse: within and without. In Azzopardi, L. et al. (eds.) Advances in Information Retrieval. 41st <lb/>European Conference on IR Research (ECIR 2019), vol. 11437 of Lecture Notes in Computer Science, 747-754, https://doi. <lb/>org/10.1007/978-3-030-15712-8_49 (Springer, Berlin Heidelberg New York, 2019). <lb/>30. Broder, A. Z. On the resemblance and containment of documents. In Carpentieri, B., Santis, A. D., Vaccaro, U. &amp; Storer, J. A. (eds.) <lb/>Compression and Complexity of SEQUENCES 1997, Positano, Amalfitan Coast, Salerno, Italy, June 11-13, 1997, Proceedings, 21-29, <lb/>https://doi.org/10.1109/SEQUEN.1997.666900 (IEEE, 1997). <lb/>31. Potthast, M. et al. Overview of the 4th international competition on plagiarism detection. In Forner, P., Karlgren, J. &amp; Womser-<lb/>Hacker, C. (eds.) Working Notes Papers of the CLEF 2012 Evaluation Labs (2012). <lb/>32. Stamatatos, E. Plagiarism detection using stopword n-grams. J. Assoc. Inf. Sci. Technol. 62, 2512-2527, https://doi.org/10.1002/ <lb/>asi.21630 (2011). <lb/>33. Gienapp, L., Kircheis, W., Bjarne, S., Stein, B. &amp; Potthast, M. Webis-STEREO-21 corpus (metadata only version). Zenodo https://doi. <lb/>org/10.5281/zenodo.5575285 (2021). <lb/></listBibl>

			<page>11 <lb/></page>

			<note place="footnote">Scientific Data | <lb/>(2023) 10:58 | https://doi.org/10.1038/s41597-022-01908-z <lb/></note>

			<note place="headnote">www.nature.com/scientificdata <lb/>www.nature.com/scientificdata/ <lb/></note>

			<listBibl>34. Potthast, M. et al. Overview of the 5th international competition on plagiarism detection. In Forner, P., Navigli, R. &amp; Tufis, D. (eds.) <lb/>Working Notes Papers of the CLEF 2013 Evaluation Labs (2013). <lb/>35. Peng, K., Mathur, A. &amp; Narayanan, A. Mitigating dataset harms requires stewardship: lessons from 1000 papers. In Vanschoren, J. &amp; <lb/>Yeung, S. (eds.) Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and <lb/>Benchmarks 2021, December 2021, virtual (2021). <lb/>36. Mieskes, M. A quantitative study of data in the NLP community. In Proceedings of the First ACL Workshop on Ethics in Natural Language <lb/>Processing, 23-29, https://doi.org/10.18653/v1/W17-1603 (Association for Computational Linguistics, Valencia, Spain, 2017). <lb/>37. Leidner, J. L. &amp; Plachouras, V. Ethical by design: ethics best practices for natural language processing. In Proceedings of the First ACL <lb/>Workshop on Ethics in Natural Language Processing, 30-40, https://doi.org/10.18653/v1/W17-1604 (Association for Computational <lb/>Linguistics, Valencia, Spain, 2017). <lb/>38. Gebru, T. et al. Datasheets for datasets. Commun. ACM 64, 86-92, https://doi.org/10.1145/3458723 (2021). <lb/>39. Potthast, M. et al. Overview of the 6th international competition on plagiarism detection. In Cappellato, L., Ferro, N., Halvey, M. &amp; <lb/>Kraaij, W. (eds.) Working Notes Papers of the CLEF 2014 Evaluation Labs, vol. 1180 of Lecture Notes in Computer Science (2014). <lb/>40. Forner, P., Karlgren, J. &amp; Womser-Hacker, C. (eds.). CLEF 2012 Evaluation Labs and Workshop-Working Notes Papers, 17-20 <lb/>September, Rome, Italy (CEUR-WS.org, 2012). <lb/>41. Forner, P., Navigli, R. &amp; Tufis, D. (eds.). CLEF 2013 Evaluation Labs and Workshop-Working Notes Papers, 23-26 September, Valencia, <lb/>Spain (CEUR-WS.org, 2013). <lb/>42. Cappellato, L., Ferro, N., Halvey, M. &amp; Kraaij, W. (eds.). Working Notes Papers of the CLEF 2014 Evaluation Labs, CEUR Workshop <lb/>Proceedings (CEUR-WS.org, 2014). <lb/></listBibl>

			<div type="acknowledgement">Acknowledgements <lb/>This work was supported by grant no. 01PW18015B of the German Federal Ministry of Education and <lb/>Research and by the Open Access Publishing Fund of Leipzig University, which is supported by the German <lb/>Research Foundation within the program Open Access Publication Funding. <lb/></div>

			<div type="annex">author contributions <lb/>L.G., M.P., W.K. and B.St. conceived the experiment. L.G. conducted the experiment. W.K. and B.Si. assembled <lb/>the metadata and carried out the plain text extraction. All authors wrote and reviewed the manuscript. <lb/></div>

			<div type="funding">Funding <lb/>Open Access funding enabled and organized by Projekt DEAL. <lb/></div>

			<div type="annex">Competing interests <lb/>The authors declare no competing interests. <lb/></div>

			<front>Additional information <lb/>Correspondence and requests for materials should be addressed to L.G. or M.P. <lb/>Reprints and permissions information is available at www.nature.com/reprints. <lb/>Publisher&apos;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and <lb/>institutional affiliations. <lb/>Open Access This article is licensed under a Creative Commons Attribution 4.0 International <lb/>License, which permits use, sharing, adaptation, distribution and reproduction in any medium or <lb/>format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-<lb/>ative Commons license, and indicate if changes were made. The images or other third party material in this <lb/>article are included in the article&apos;s Creative Commons license, unless indicated otherwise in a credit line to the <lb/>material. If material is not included in the article&apos;s Creative Commons license and your intended use is not per-<lb/>mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the <lb/>copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. <lb/>© The Author(s) 2023 </front>


	</text>
</tei>
