<?xml version="1.0" ?>
<tei xml:space="preserve">
	<teiHeader>
		<fileDesc xml:id="0"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Articles <lb/>https://doi.org/10.1038/s41562-022-01351-5 <lb/>1 <lb/>Department of Sociology, Queens College, City University of New York, New York, NY, USA. 2 Department of Sociology, University of California, <lb/>Los Angeles, CA, USA. 3 Institute for Social Science Research (IRiSS) at Stanford University, Stanford, CA, USA. ✉ e-mail: charles.gomez@qc.cuny.edu <lb/></front>

			<body>I <lb/>nternational inequalities have the potential to disrupt the gen-<lb/>eration and flow of knowledge in the global scientific commu-<lb/>nity. Research can make an impact only if it is visible. Of course, <lb/>journals and scientific reputations shape the visibility of research <lb/>in conjunction with the overall quality of a study. But so too do <lb/>national scientific infrastructures and reputations. The scale and <lb/>quality of research, and even library subscription practices, are <lb/>heavily influenced by the amount of funding countries set aside <lb/>for the scientific enterprise 1-4 . Countries with these advantages <lb/>probably receive additional citations for their research, over and <lb/>above what one would expect just from the subject matter of that <lb/>research 5 , something that scholars of the Global South have sug-<lb/>gested before 2,6,7 . Studying these inequalities systematically is diffi-<lb/>cult because the many relevant factors are so deeply intertwined. In <lb/>this paper, we introduce a framework we call citational lensing that <lb/>takes advantage of the strong relationship between citations and <lb/>textual similarity to identify countries that receive more (or fewer) <lb/>citations than one would expect if citations and textual similarity <lb/>were perfectly aligned. <lb/>Much like how gravity distorts our perception of light, national <lb/>factors distort our perception of international science. The intu-<lb/>ition behind our approach is to think of international science as a <lb/>multiplex network, with the citations running between countries <lb/>representing one type of connection, and the textual similarity of <lb/>their research output representing another. Citational lensing is <lb/>measured as the difference between the weighted edges that cor-<lb/>respond to international citations and the weighted edges that cor-<lb/>respond to textual similarity. The result is a set of connections that <lb/>represent how much more one country cites another, relative to <lb/>what we would expect if citations and textual similarity reflected <lb/>each other perfectly. We call this layer of the multiplex network <lb/>the citational well. Filtering out the effect of other factors is also <lb/>possible, leaving distortion captured in the citational well to repre-<lb/>sent a more narrowly defined notion of inequality. This is shown in <lb/>Fig. 1, where we represent science as a multiplex network with three <lb/>layers: L citation is the citation network between countries containing <lb/>the citation flow from country i to country j, L T <lb/>text is the relative sim-<lb/>ilarity of the text of country j&apos;s research output to that of country i <lb/>by applying a Kullback-Leibler divergence (KLD) measure 8 to the <lb/>distinct national signatures of countries i and j produced by a super-<lb/>vised topic model called a labelled latent Dirichlet allocation (LDA) <lb/>model 9 , and L distortion is what we call the citational well defined as the <lb/>difference between the two layers: <lb/>L distortion = L citation -L T <lb/>text <lb/>(1) <lb/>L distortion represents the distortion in the citations from country i <lb/>to country j, relative to what we would expect given the similarity of <lb/>the research written by scientists in these two countries. <lb/>Our approach builds on the long-standing tradition in the sci-<lb/>ence of science that uses citation networks and text analysis of <lb/>scientific papers to embody the flow of ideas in science and map <lb/>its structure, as well as the distribution and spread of knowledge <lb/>within it 5,10-14 . Yet the citation networks and the textual similarity <lb/>between fields are not always aligned. There are commonly more <lb/>citations between fields than we would expect on the basis of the <lb/>textual similarity of their papers, or conversely, more similarity in <lb/>the text than we would expect given the number of citations flowing <lb/>between those fields 12,15 . <lb/>It is not clear whether this misalignment between citations and <lb/>textual similarity has any substantive importance for scientific <lb/>fields 16 . Bibliometrics has treated the issue as a question of ground <lb/>truth, where the differences between the two are less important than <lb/>their respective differences from a third, external criterion 17,18 . In <lb/>this line of thinking, the misalignment of citations and textual simi-<lb/>larity is simply beside the point and does not impact the larger goal <lb/>of mapping science. In the science of science, meanwhile, the mis-<lb/>alignment is taken as a sign that any model of diffusion or commu-<lb/>nication between scientific fields needs to take both citations and <lb/>textual similarity into account 11,12,15 . <lb/>When it comes to countries, misalignments between citations <lb/>and textual similarity carry practical significance. This is because <lb/></body>

			<front>Leading countries in global science increasingly <lb/>receive more citations than other countries doing <lb/>similar research <lb/>Charles J. Gomez 1 ✉ , Andrew C. Herman 2 and Paolo Parigi 3 <lb/>Citations and text analysis are both used to study the distribution and flow of ideas between researchers, fields and countries, <lb/>but the resulting flows are rarely equal. We argue that the differences in these two flows capture a growing global inequality <lb/>in the production of scientific knowledge. We offer a framework called &apos;citational lensing&apos; to identify where citations should <lb/>appear between countries but are absent given that what is embedded in their published abstract texts is highly similar. This <lb/>framework also identifies where citations are overabundant given lower similarity. Our data come from nearly 20 million papers <lb/>across nearly 35 years and 150 fields from the Microsoft Academic Graph. We find that scientific communities increasingly <lb/>centre research from highly active countries while overlooking work from peripheral countries. This inequality is likely to pose <lb/>substantial challenges to the growth of novel ideas. <lb/>NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></front>

			<page>919 <lb/></page>
            
            <note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

            <body>they represent the combined effect of several factors, including <lb/>the overall quality of research and national reputations. The cita-<lb/>tional well provides a noisy signal that can be further refined to <lb/>better approximate specific factors, as we show with respect to <lb/>national reputation. <lb/>Identifying citational distortions is critical to discussions on <lb/>inequality in global scientific knowledge production. Knowledge <lb/>production is overwhelmingly skewed towards resource-wealthy <lb/>countries such as the United States and those in Western Europe <lb/>and East Asia (to name a few) that house the best universities, <lb/>Nobel winners and journal editors, so identifying undercited coun-<lb/>tries both promotes the inclusion of often-excluded voices and <lb/>helps foster the scientific enterprises of these countries. The type of <lb/>distortion we consider here is also likely to be problematic for sci-<lb/>entific progress if knowledge remains unincorporated and human <lb/>capital unused. <lb/>results <lb/>To illustrate how citational lensing can be applied, we use roughly <lb/>20 million academic papers in nearly 150 fields and subfields <lb/>from 1980 to 2012 in the Microsoft Academic Graph (MAG), one <lb/>of the most extensive metadata repositories of academic publica-<lb/>tions. These data include metadata such as citations, along with the <lb/>abstract text of published research articles. We show how citational <lb/>lensing can be used to characterize changes in the international <lb/>scientific hierarchy over time and how it can be scaled to cover all <lb/>of science. <lb/>Citations and recognition. In Fig. 2a, we count for each year the <lb/>number of countries that are present in both the international text <lb/>similarity network and the international citation network for each <lb/>field and for each year from 1980 to 2012. The plot captures the <lb/>distribution of fields in each year, as well as the overall average <lb/>number of countries represented in fields for each year. Across all <lb/>types of fields, the number of countries represented in the global <lb/>scientific conversation is increasing, as captured by journal arti-<lb/>cles. Note that while Fig. 2a shows steady growth in the number <lb/>of countries in the international scientific community over time, <lb/>this appears to taper off in the years immediately prior to 2012. In <lb/>Supplementary Tables 3 and 4, we test how much variance in our <lb/>citational distortion and thus our text similarity measures is due to <lb/>the result of the sheer volume of papers produced by authors from <lb/>countries using hierarchical linear models. The mediating impact <lb/>of N papers is modest. Adjusting for N papers reduces the vari-<lb/>ance in country-related text similarity by 19%, from 0.16 to 0.13. <lb/>Likewise, adjusting for N papers reduces the country-related vari-<lb/>ance in citational distortion by 12.5%, from 0.08 to 0.07. <lb/>While there may be more countries participating in global sci-<lb/>ence, the extent to which their work is visible and valued may still be <lb/>highly stratified. The relationship between the textual similarity of <lb/>countries&apos; research output and the number of citations they receive <lb/>is one simple way to think about this. In a world where all research <lb/>is equally visible and equally valued, we would expect an extremely <lb/>close relationship between the similarity of research in their text and <lb/>the citations that flow between them. We test the extent to which <lb/>L citation corresponds with L text across fields and over time in Fig. 2b. <lb/>This is done using a form of network regression model called the <lb/>semi-partialing quadratic assignment procedure (QAP) 19 to capture <lb/>the extent to which citations are associated with L T <lb/>text . Specifically, <lb/>we run two types of QAP models based on which country nodes are <lb/>included in L text and L citation , the first using all countries present in the <lb/>data that year and the second using only &apos;core&apos; scientific countries-<lb/>specifically, those in Western Europe and East Asia (that is, China, <lb/>Japan and South Korea), alongside the United States, Canada, <lb/>Australia, New Zealand, Singapore and Israel. (In Supplementary <lb/>Table 2, we itemize every country we use in our analysis, parsed by <lb/>their core and periphery classification.) These countries are com-<lb/>monly considered to be the leaders in scientific research (housing <lb/>the best universities and most-cited scientists, publishing work in <lb/>leading outlets and so on) and as such form the core of global sci-<lb/>entific communities where most scientific activity takes place 20,21 . <lb/>We run each type of QAP model for every field in every year, <lb/>from 1980 to 2012. The dependent variable in each model is the <lb/>network represented in L T <lb/>text for that field and year, while the inde-<lb/>pendent variable is the international citation network for that field <lb/>and year, given as L citation . We thus regress how similar country j is to <lb/>country i in L T <lb/>text on how much country j cites country i. There are <lb/>two &apos;grand average&apos; trends in Fig. 2b: the average β coefficient value <lb/>across all fields in each year, and the shaded area in the plot, which <lb/>is the grand standard errors across all βs in each year. Note that <lb/>we only consider βs that are statistically significant at a two-tailed <lb/>P value threshold of 0.05. <lb/>There are a few notable take-aways from these trends. A <lb/>one-standard-deviation increase in the number of citations, as <lb/>defined for each QAP model in the year 2012, is associated with <lb/>0.228-standard-deviation-higher KLD scores in L text among all <lb/>countries (that is, an all-inclusive model with both core and periph-<lb/>ery countries) (N = 135; 95% confidence interval (CI), (0.226, <lb/>0.231); two-tailed t-tests). However, when we compare the QAP <lb/>model with just core countries against the QAP model that includes <lb/>all countries (that is, core and periphery), we find that citations <lb/>have a consistent and strong relationship with the similarity of lan-<lb/>guage in international research, where a one-standard-deviation <lb/>increase in the number of citations in 2012 is associated with <lb/>0.312-standard-deviation-higher KLD scores in L text for just core <lb/>countries (N = 139; 95% CI, (0.309, 0.315); two-tailed t-tests). This <lb/>is also evidenced by the overall higher average β values that remain <lb/>fairly steady over time, albeit with a slight increase in the same <lb/>period within the core-country QAP model. The average β value for <lb/>the core-periphery model, by contrast (which reflects the relation-<lb/>ship across all countries in the dataset), is not only lower overall than <lb/>that for the core country model but is also weakening over time. <lb/>(In Supplementary Figs. 8 and 9, we rerun the QAP models to now <lb/>include countries&apos; β coefficients that include only peripheral coun-<lb/>tries, similar to the core network that contains only core countries.) <lb/>Figure 3 plots the average citational distortion (that is, the aver-<lb/>age in-degree centrality) that each country experiences in L distortion . <lb/>This should be interpreted as the difference in the number of stan-<lb/>dard deviations between edges in the citation network (measured in <lb/>i <lb/>i <lb/>j <lb/>L citation <lb/>j <lb/>L distortion <lb/>L T <lb/>text <lb/>i <lb/>j <lb/>Fig. 1 | the construction of the multiplex network, the citational well. <lb/>The citational well is the result of subtracting the edge weights of the <lb/>international text similarity network from the corresponding edge weights <lb/>of the international citation network. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>920 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

			<body>terms of standard deviations relative to the citation network) and <lb/>those in the text similarity network (measured in terms of standard <lb/>deviations relative to the text similarity network). Figure 3a high-<lb/>lights many of the countries with the greatest positive distortions. <lb/>Figure 3b shows the average citational distortion over time among <lb/>core countries and among periphery countries. (As the United <lb/>States is an outlier, we remove it from the average core trend in <lb/>Fig. 3b and all subsequent in-degree plots.) The United States is the <lb/>most central country in the citation networks, for all fields and over <lb/>time. The deviance of the United States&apos; centrality in L distortion sug-<lb/>gests that it is, on average, highly overcited. This holds true for other <lb/>power players in global science, such as Germany, the Netherlands, <lb/>the United Kingdom and Japan. The other major trend is that China <lb/>rises considerably over the past few decades, from being undercited <lb/>throughout the 1980s and early 1990s to being overcited in the <lb/>2000s, even quickly approaching many countries in Western Europe. <lb/>As shown in Fig. 3b, the gap between core and periphery countries <lb/>is growing substantially over time, as core countries are increasingly <lb/>overcited for their work relative to what they study, while peripheral <lb/>countries are increasingly undercited for their work. (As the grow-<lb/>ing gap between the trends in core and periphery countries may <lb/>be the result of the number of countries, in Supplementary Figs. 28 <lb/>and 29, we censor them on the basis of when countries first appeared <lb/>in the data for each field, finding that our results still hold.) <lb/>20 <lb/>40 <lb/>60 <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>Year <lb/>Number of countries <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>a <lb/>0.24 <lb/>0.26 <lb/>0.28 <lb/>0.30 <lb/>0.32 <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>Year <lb/>β coefficient <lb/>(standard deviations) <lb/>Core + periphery <lb/>Core <lb/>b <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>0.20 <lb/>0.25 <lb/>0.30 <lb/>0.35 <lb/>Year <lb/>β coefficient <lb/>(standard deviations) <lb/>Core + periphery <lb/>Core <lb/>c <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>Fig. 2 | relationship between citations and text over time. a, The average number of countries present in both the international text similarity network and <lb/>the international citation network over time from 1980 to 2012, with trend lines for each field type. b, The average of statistically significant β coefficients <lb/>from each field&apos;s yearly QAP model for citations plotted on the y axis and over time on the x axis from 1980 to 2012. The shading around the trends <lb/>denotes the standard errors across fields. c, The trends plotted in b, but parsed by the type of field. The shading around the trends denotes the grand <lb/>standard errors across β coefficients. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>921 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

			<body>Figure 4a expands the plots in Fig. 3a, while Fig. 4b expands the <lb/>plots in Fig. 3b, but the trends are now averaged by research area. <lb/>While the most overcited countries are still present across fields, the <lb/>ordering differs. The United States remains the most central across <lb/>all fields. China, however, has overtaken some European countries <lb/>in the physical and mathematical sciences and in the engineering <lb/>and computational sciences. In the biomedical, behavioural and eco-<lb/>logical sciences, China is nearing Germany but trailing the United <lb/>Kingdom. The exception is the social sciences, where China is on <lb/>par with Germany but noticeably trails the United Kingdom. When <lb/>comparing core and periphery countries in Fig. 4b by research area, <lb/>the gap is most pronounced in the physical and mathematical sci-<lb/>ences, followed by the engineering and computational sciences and <lb/>the biomedical, behavioural and ecological sciences. The gap has <lb/>only recently emerged in the social sciences. <lb/>A stagnating hierarchy. Thus far, we have focused on the most <lb/>overcited distorted countries across different fields and over time. <lb/>However, most countries are undercited. With some exceptions, <lb/>countries that are overcited or undercited remain constant over <lb/>time. To unpack this, Fig. 5 plots the average distortion for several <lb/>countries at two points in time-the year 2000 on the x axis and <lb/>the year 2012 on the y axis-but parsed by research area and in <lb/>four transnational regions: (1) Europe, (2) Asia, (3) Africa and the <lb/>Middle East and (4) Latin America and the Caribbean. First, with-<lb/>out any divides by research area or transnational region as shown <lb/>here, all countries cluster very closely near the y = x parity line <lb/>(with a Pearson&apos;s product-moment correlation of ρ = 0.659; t = 9.59; <lb/>d.f. = 120; 95% CI, (0.545, 0.749); P &lt; 2.2 × 10 -16 ). Countries tend to <lb/>cluster in one of two quadrants: overcited in both 2000 and 2012 <lb/>(that is, data points that are positive in both years) or undercited <lb/>in both 2000 and 2012 (that is, data points that are negative in both <lb/>years). Overcited countries are typically part of the core of global <lb/>science. There are far fewer countries in the overcited quadrant than <lb/>in the undercited quadrant, which comprises the periphery of global <lb/>science. Except the power players in global science, most countries <lb/>seem to be under-recognized for their work. In other words, most <lb/>countries remain in either the lower left quadrant or the upper right <lb/>quadrant, indicating that countries do not generally change their <lb/>station in their distortion over time. <lb/>That being said, note as well that some regions and research areas <lb/>are more stable than others in terms of the average citational distor-<lb/>tion for countries across the two periods. Unsurprisingly, Europe <lb/>shows high correlations for all research areas (ρ = 0.68; t = 5.01; <lb/>d.f. = 29; 95% CI, (0.431, 0.834); P &lt; 2.47 × 10 -5 for biomedical, <lb/>behavioural and ecological sciences; ρ = 0.66, t = 5.00; d.f. = 32; <lb/>95% CI, (0.418, 0.817); P &lt; 1.99 × 10 -5 for engineering and compu-<lb/>tational sciences; ρ = 0.76; t = 7.13; d.f. = 37; 95% CI, (0.586, 0.868); <lb/>P &lt; 1.92 × 10 -8 for physical and mathematical sciences; ρ = 0.66; <lb/>t = 4.27; d.f. = 23; 95% CI, (0.366, 0.839); P &lt; 2.89 × 10 -4 for social <lb/>sciences). Other regions show more variation across research areas. <lb/>While citational distortions in engineering and computational sci-<lb/>ences are highly correlated in Asia, with lower correlations in other <lb/>research areas, Latin America and the Caribbean show a different <lb/>pattern, with a high correlation in the biomedical, behavioural and <lb/>ecological sciences and lower correlations in other research areas. <lb/>By contrast, Africa and the Middle East have a lower (albeit sta-<lb/>tistically significant) correlation in the engineering and computa-<lb/>tional sciences, but lower and non-significant correlations in other <lb/>research areas. <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>2020 <lb/>Year <lb/>Average citational distortion <lb/>(standard deviations) <lb/>a <lb/>Average citational distortion <lb/>(standard deviations) <lb/>b <lb/>0.05 <lb/>0 <lb/>0.05 <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>Year <lb/>Periphery <lb/>Core <lb/>China <lb/>Germany <lb/>Japan <lb/>Netherlands <lb/>Switzerland <lb/>United Kingdom <lb/>United States <lb/>Fig. 3 | Comparing global citational distortion over time. a, The average national citational distortion in L distortion plotted across fields on the y axis and over <lb/>time on the x axis for select countries. b, The average national citational distortion plotted for core countries and peripheral countries on the y axis and over <lb/>time on the x axis. The gap in the average distortion in the citational well is growing between core and periphery countries. The shading around the trends <lb/>denotes the standard errors of these averages. Note that citational distortion should be interpreted as the difference in the number of standard deviations <lb/>between edges in the citation network (measured in terms of standard deviations relative to the edge weights in the citation network) and those in the text <lb/>similarity network (measured in terms of standard deviations relative to the edge weights in the text similarity network). <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>922 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

			<body>Figure 6 plots the percentage of countries in the core and in the <lb/>periphery that are being overcited or undercited in the year 2000 <lb/>and in the year 2012 for different types of fields. Here the denomi-<lb/>nator is the number of countries present in the year 2000 or the year <lb/>2012, where summing the percentages vertically in each year results <lb/>in 100%. The data points in the figure also contain the number of <lb/>countries (given as N) and the average distortion value with its stan-<lb/>dard errors in parentheses. <lb/>Except for the physical and mathematical sciences, where the <lb/>representation of countries in the overcited and undercited groups <lb/>remained steady, the percentage of core countries that were over-<lb/>cited increased from 2000 to 2012. For the biomedical, behavioural <lb/>and ecological sciences, the percentage increased from 9.68% of all <lb/>countries in 2000 to 22.73% of all countries in 2012; for the engi-<lb/>neering and computational sciences, from 13.64% of all countries <lb/>to 22.73% of all countries; and for the social sciences, from 2.27% <lb/>of all countries to 9.09% of all countries. Similarly, the percentage <lb/>of periphery countries that were undercited rose over the same <lb/>period. For the biomedical, behavioural and ecological sciences, <lb/>the percentage increased from 40.32% of all countries in 2000 to <lb/>43.55% of all countries in 2012; for the engineering and compu-<lb/>tational sciences, from 39.83% of all countries to 45.76% of all <lb/>countries; and for the social sciences, from 36.9% of all countries <lb/>to 42.86% of all countries. Notice as well that the changing repre-<lb/>sentation of countries within the overcited and undercited groups <lb/>does not necessarily correspond to the average distortion numbers <lb/>within those groups. <lb/>Figure 7 uses the average distortion of each country in 2012 by <lb/>field type to calculate transnational regional averages to include <lb/>the four regions mentioned previously, as well as North America <lb/>(the United States and Canada) and Oceania (notably including <lb/>Australia and New Zealand). This figure was plotted in R using <lb/>the package maps 22 . We created global maps based on the average <lb/>distortion of countries within transnational regions in the year <lb/>2012 for the different types of fields. Figure 8 expands this map <lb/>by plotting the average national distortion within each region and <lb/>parsed by the type of field. This figure was also plotted using the <lb/>package maps 22 . Note that for the transnational regional figures in <lb/>Fig. 8a-d, the colour scales are mapped for the data of countries in <lb/>those regions. Countries should not be compared in their colours <lb/>across figures. The global map in Fig. 7 highlights the main regional <lb/>differences for that purpose. There are several notable countries in <lb/>each region, such as China in Fig. 8b and Brazil in Fig. 8c. In con-<lb/>trast, countries in Africa and the Middle East and most countries <lb/>in South America are near parity, where they are cited to the same <lb/>degree to which their research language aligns to other countries. <lb/>China <lb/>Germany <lb/>Japan <lb/>Netherlands <lb/>Switzerland <lb/>United Kingdom <lb/>United States <lb/>China <lb/>Germany <lb/>Japan <lb/>Netherlands <lb/>Switzerland <lb/>United Kingdom <lb/>United States <lb/>China <lb/>Germany <lb/>Japan <lb/>Netherlands <lb/>Switzerland <lb/>United Kingdom <lb/>United States <lb/>China <lb/>Germany <lb/>Japan <lb/>Netherlands <lb/>Switzerland <lb/>United Kingdom <lb/>United States <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>1980 <lb/>2000 <lb/>2020 <lb/>1980 <lb/>2000 <lb/>2020 <lb/>1980 <lb/>2000 <lb/>2020 <lb/>1980 <lb/>2000 <lb/>2020 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>1.5 <lb/>Year <lb/>Average citational distortion <lb/>(standard deviations) <lb/>a <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 1980 <lb/>1990 <lb/>2000 <lb/>2010 <lb/>0.10 <lb/>0.05 <lb/>0 <lb/>0.05 <lb/>0.10 <lb/>0.15 <lb/>Year <lb/>Average citational distortion <lb/>(standard deviations) <lb/>Periphery <lb/>Core <lb/>b <lb/>Fig. 4 | Comparing global citational distortion by the type of field. a,b, The trends plotted in Fig. 3, but parsed by the type of field. The shading around <lb/>the trends denotes the standard errors of these averages. Note that citational distortion should be interpreted as the difference in the number of standard <lb/>deviations between edges in the citation network (measured in terms of standard deviations relative to the edge weights in the citation network) and those <lb/>in the text similarity network (measured in terms of standard deviations relative to the edge weights in the text similarity network). <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>923 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

            <body>Discussion <lb/>These results are especially interesting in light of the broader lit-<lb/>erature. The rise of China, for instance, has been noted with other <lb/>bibliometric data 23-25 , so for the country to also be experiencing an <lb/>increase in citational distortion adds another element to this his-<lb/>tory. However, our results do not similarly re-affirm the temporal <lb/>trends around Europe in recent work on citation inequality among <lb/>elite researchers. Where that work showed that countries such as <lb/>the Netherlands and Switzerland have a large and increasing share <lb/>of elite researchers 5 , Fig. 3a shows that while the Netherlands and <lb/>Switzerland are increasingly reaping citations in excess of the textual <lb/>similarity of their research, they fall far closer to the mean. Finally, it <lb/>is noteworthy that the gap between countries with high levels and low <lb/>levels of citational distortion is most pronounced in the physical sci-<lb/>ences, considering that these fields are traditionally known for shar-<lb/>ing the strongest sense of how to evaluate and integrate knowledge 26 . <lb/>The main limitation of the citational lensing framework is one of <lb/>measurement. Textual similarity between countries is an unavoid-<lb/>ably noisy signal, and this affects the comparison with citations <lb/>downstream. So, even though it is correct to say that the international <lb/>inequalities revealed in our analyses are a matter of prominence and <lb/>recognition, more precision will come only with future refinements <lb/>to the methodology. We have been able to correct for citation infla-<lb/>tion in Supplementary Figs. 10-15, and in Supplementary Figs. 16-21 <lb/>we describe the results of a secondary analysis where we test our <lb/>rudimentary controls for the quality of research by uncensoring <lb/>journal selection. However, there are other factors that matter and <lb/>could always be taken into account in the future. <lb/>These potential concerns are offset by a number of unique <lb/>strengths that the citational lensing framework provides. One of <lb/>the primary ones we have outlined is its adaptability. We have used <lb/>nation-labelled LDA (NL-LDA) in tandem with the KLD to model <lb/>the similarity of scientific text, but many other approaches could <lb/>be used in their place to capture another nuance around language <lb/>use in science. The entropy-based metrics advocated by Vilhena <lb/>et al. 12 and Altmann et al. 13 would bring more attention to ineffi-<lb/>ciencies in international communication among scientists, to take <lb/>one example. <lb/>R = 0.32 <lb/>P = 0.25 <lb/>R = 0.75 <lb/>P = 0.0014 <lb/>R = 0.52 <lb/>P = 0.034 <lb/>R = 0.14 <lb/>P = 0.66 <lb/>R = 0.68 <lb/>P = 2.5 × 10 -5 <lb/>R = 0.66 <lb/>P = 2 × 10 -5 <lb/>R = 0.76 <lb/>P = 1.9 × 10 -8 <lb/>R = 0.66 <lb/>P = 0.00029 <lb/>R = 0.75 <lb/>P = 0.0045 <lb/>R = 0.27 <lb/>P = 0.52 <lb/>R = 0.46 <lb/>P = 0.13 <lb/>R = 0.4 <lb/>P = 0.28 <lb/>R = -0.16 <lb/>P = 0.42 <lb/>R = 0.46 <lb/>P = 0.025 <lb/>R = 0.25 <lb/>P = 0.19 <lb/>R = 0.27 <lb/>P = 0.27 <lb/>Asia <lb/>Europe <lb/>Latin America <lb/>and the Caribbean <lb/>Africa and the <lb/>Middle East <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>0.5 1.0 <lb/>0.5 <lb/>0 <lb/>0.5 1.0 <lb/>0.5 <lb/>0 <lb/>0.5 1.0 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>Average citational distortion (standard deviations) <lb/>2000 <lb/>Average citational distortion (standard deviations) <lb/>2012 <lb/>Fig. 5 | Comparing citational distortion between 2000 and 2012. The average distortion for countries in the year 2012, parsed by region and by the type <lb/>of discipline. The trends for all countries in the years 2000 and 2012 are plotted on the x axis and y axis, respectively, but parsed by the type of field and by <lb/>transnational region. R refers to the Pearson&apos;s product-moment correlation. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>924 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

            <body>2000 <lb/>2012 <lb/>2000 <lb/>2012 <lb/>2000 <lb/>2012 <lb/>2000 <lb/>2012 <lb/>5.0 <lb/>10.0 <lb/>15.0 <lb/>20.0 <lb/>30.0 <lb/>35.0 <lb/>40.0 <lb/>45.0 <lb/>Percentage of countries (%) <lb/>Periphery <lb/>Core <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>Overcited <lb/>Undercited <lb/>9.09% N = 4 <lb/>0.13 (0.04) <lb/>9.68% N = 12 <lb/>0.13 (0.04) <lb/>22.73% N = 10 <lb/>0.13 (0.05) <lb/>6.45% N = 8 <lb/>0.14 (0.07) <lb/>40.91% N = 18 <lb/>0.1 (0.02) <lb/>40.32% N = 50 <lb/>0.17 (0.02) <lb/>27.27% N = 12 <lb/>0.07 (0.01) <lb/>43.55% N = 54 <lb/>0.15 (0.01) <lb/>13.64% N = 6 <lb/>0.11 (0.04) <lb/>10.17% N = 12 <lb/>0.12 (0.04) <lb/>22.73% N = 10 <lb/>0.18 (0.11) <lb/>4.24% N = 5 <lb/>0.08 (0.07) <lb/>36.36% N = 16 <lb/>0.08 (0.01) <lb/>39.83% N = 47 <lb/>0.19 (0.02) <lb/>27.27% N = 12 <lb/>0.09 (0.01) <lb/>45.76% N = 54 <lb/>0.16 (0.02) <lb/>18.18% N = 8 <lb/>0.23 (0.06) <lb/>10.14% N = 15 <lb/>0.15 (0.04) <lb/>18.18% N = 8 <lb/>0.3 (0.12) <lb/>10.14% N = 15 <lb/>0.09 (0.03) <lb/>31.82% N = 14 <lb/>0.12 (0.02) <lb/>39.86% N = 59 <lb/>0.17 (0.02) <lb/>31.82% N = 14 <lb/>0.06 (0.01) <lb/>39.86% N = 59 <lb/>0.17 (0.02) <lb/>2.27% N = 1 <lb/>0.03 (NA) <lb/>13.1% N = 11 <lb/>0.12 (0.04) <lb/>9.09% N = 4 <lb/>0.15 (0.05) <lb/>7.14% N = 6 <lb/>0.17 (0.1) <lb/>47.73% N = 21 <lb/>0.08 (0.01) <lb/>36.9% N = 31 <lb/>0.19 (0.02) <lb/>40.91% N = 18 <lb/>0.08 (0.01) <lb/>42.86% N = 36 <lb/>0.13 (0.01) <lb/>Year <lb/>Fig. 6 | Comparing citational distortion between core and periphery countries. The percentage of countries in the core or periphery that are either overcited <lb/>or undercited in the years 2000 and 2012, along with the average distortion for each group (standard errors are given in parentheses). Note: NA means <lb/>not available. <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>0 <lb/>0.2 <lb/>0.4 <lb/>0.6 <lb/>Average citational <lb/>distortion (standard deviations) <lb/>Fig. 7 | Comparing citational distortion by transnational region in 2012. The average distortion of countries mapped within transnational regions-Africa <lb/>and the Middle East, Latin America and the Caribbean, Asia, North America, and Oceania-and by the type of field. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>925 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

			<body>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>0.3 <lb/>0 <lb/>0.3 <lb/>0.6 <lb/>Average citational <lb/>distortion (standard <lb/>deviations) <lb/>a <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>0.5 <lb/>0 <lb/>0.5 <lb/>1.0 <lb/>Average citational <lb/>distortion (standard <lb/>deviations) <lb/>b <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>0.50 <lb/>0.25 <lb/>0 <lb/>0.25 <lb/>Average citational <lb/>distortion (standard <lb/>deviations) <lb/>c <lb/>Biomedical, behavioural <lb/>and ecological sciences <lb/>Engineering and <lb/>computational sciences <lb/>Physical and <lb/>mathematical sciences <lb/>Social <lb/>sciences <lb/>0.50 <lb/>0.25 <lb/>0 <lb/>0.25 <lb/>0.50 <lb/>Average citational <lb/>distortion (standard <lb/>deviations) <lb/>d <lb/>Fig. 8 | Citational distortion within transnational regions in 2012. a-d, The average distortion for individual countries in Europe (a), Asia (b), Latin <lb/>America and the Caribbean (c), and Africa and the Middle East (d) in 2012. Note that the colour scale varies by region to highlight regional trends that are <lb/>not visible when using a global colour scale. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>926 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

            <body>The citational well may also be meaningful in contexts where <lb/>nation-states are not the main feature of interest. Reputation effects <lb/>among journals or universities, for instance, could be studied with <lb/>the method we have described. The comparison may also be rele-<lb/>vant to questions surrounding innovation and intellectual property, <lb/>as patents are themselves embedded within a citation network and <lb/>can be evaluated on the similarity of their language use. Any exten-<lb/>sion will have to be validated, but the simplicity of the method lends <lb/>itself well to re-use in new contexts. <lb/>Regardless, identifying countries that do not receive citations <lb/>in the amount we would expect given the subject matter of their <lb/>research-whether they receive more citations or fewer-provides <lb/>us with a tool to check on the efficiency of knowledge flow in sci-<lb/>ence. Indeed, as demonstrated in Figs. 3 through 8, only a few <lb/>highlighted countries are the real sole winners here, as the overall <lb/>average β coefficient trend plotted in Fig. 2b and the overall average <lb/>citational distortion trend plotted in Fig. 3b are quite modest over <lb/>30 years. This holds real value for the study of scientific research. In <lb/>the first place, citational lensing at the international level makes it <lb/>possible to identify countries with a successful scientific enterprise. <lb/>Citations should not be the sole arbiter for success in science, though <lb/>the reality is that they continue to be disproportionately impactful <lb/>as a metric. Countries that seem overcited by the approach we have <lb/>taken here have been successful in this narrow sense. The empiri-<lb/>cal results bear out these points. Consider the United States and <lb/>China: both countries reap far more citations than the language of <lb/>their work would suggest. While this has been true for the United <lb/>States for a long time, reflective of its large economy and substantial <lb/>investment in scientific research, China&apos;s success in attracting cita-<lb/>tions has come more recently. More work would be needed to estab-<lb/>lish any causal connection, but the timing of China&apos;s rise in Figs. <lb/>3a, 4a and 8b is conspicuous given its major policy shift regarding <lb/>science and technology in the 1990s. <lb/>This leaves citational lensing as a useful metric in tracking the <lb/>effectiveness of national science policies, as well as evaluating the <lb/>relative importance of various national factors in nurturing a highly <lb/>cited scientific community. Otherwise, citational distortions will <lb/>continue to impose limits on the circulation of knowledge, novel <lb/>ideas and future innovations and are ultimately inefficient for sus-<lb/>tained knowledge production. Better identifying who is undercited <lb/>not only promotes the inclusion of often excluded perspectives but <lb/>also enhances knowledge production. The exposure to diverse per-<lb/>spectives and abilities consistently improves outcomes in collective <lb/>problem-solving ventures such as scientific research collabora-<lb/>tions 27 . At the very least, overlooking research from wide swaths <lb/>of the global scientific community means that knowledge remains <lb/>unincorporated and human capital unused, especially in many ris-<lb/>ing middle-income countries with growing scientific enterprises. <lb/>Citational lensing also offers a way to study the differences in <lb/>knowledge production between the Global South and the Global <lb/>North, and the flow of knowledge between them. The long-standing <lb/>concerns around the global inequalities in science have typically <lb/>been substantiated with analyses of publication patterns or with rich <lb/>qualitative interviews 2,28-31 . By bringing attention to the (mis)match <lb/>between citations and textual similarity, citational lensing can help <lb/>reveal more of the true impact that countries imprint on discourses <lb/>across scientific fields and on the disproportionate attention and <lb/>recognition that some research receives. Progress has been made by <lb/>several countries in the semi-periphery towards this end, but each <lb/>new entrant to the international scientific community continues to <lb/>face a struggle in getting the appropriate recognition for the work of <lb/>their scientists. That said, individual scientists probably derive ben-<lb/>efits in their career trajectories from national reputations, though <lb/>more research needs to be done to confirm this. <lb/>What remains to be seen is which factors do the most to dis-<lb/>tort citations from textual similarity. The visibility and quality of <lb/>research are both likely contributors, alongside funding levels and <lb/>overall reputations at the national level. However, as we do control <lb/>for the growth in the number of journals over time, this may be <lb/>one way to account for both the quality of published work and its <lb/>subsequent visibility. Similarly unsettled is the issue of whether the <lb/>distortion captured by the citational well could predict or stand in <lb/>as a proxy for any of these same factors 32-34 . <lb/>methods <lb/>To capture citational lensing, we represent science as a multiplex network, L, with <lb/>three layers (Fig. 1). Consider the simple case of citational lensing in a single field <lb/>in a given year t. L citation is the citation network between countries, where Lcitation i,j <lb/>contains the citation flow from country i to country j. To make things comparable <lb/>across the layers of the multiplex network, L citation is constructed as the number <lb/>of citations received by country i&apos;s papers published in the given field in year t by <lb/>all other countries j, where we use a five-year window after publication year t to <lb/>capture all citations from countries j from year t to year t + 5. In that way, the text <lb/>network based on published papers in year t corresponds to the citation network <lb/>of the number of cumulative citations received over the ensuing five years by <lb/>papers published in year t. We use z-scores for the edge weights rather than the <lb/>raw citation counts themselves. <lb/>Another layer, L text , is a network where each connection Ltext i,j is the similarity <lb/>of the text of country i&apos;s research output to that of country j. To capture the degree <lb/>of similarity, we apply a unique supervised topic model called a labelled LDA <lb/>model 9 . Using the nationalities of authors on papers, the NL-LDA model is unique <lb/>in that it captures the extent to which ideas and concepts embodied by n-grams in <lb/>the texts are associated to authors from which countries. This approach is useful <lb/>to disentangle and establish what is being studied in different countries, as many <lb/>papers are increasingly authored by researchers from different countries. The KLD 8 <lb/>is taken for the similarity between countries in the text of their scientific papers. In <lb/>our case, the KLD measures how much information is lost going from the text of <lb/>one country i&apos;s scientific output to that of another country j. <lb/>The reasoning here is similar to that used in other work in the science of <lb/>science 12 . Information loss imitates the amount of work that scholars have to do <lb/>to communicate their ideas. When very little information is lost, communication <lb/>is seamless; when lots of information is lost, communication is difficult. Note <lb/>that this is not a symmetrical relationship, and that is by design. The L T <lb/>text layer <lb/>tends to identify the most common subject matter in national research, so <lb/>when information is lost in moving from country i to country j, it indicates that <lb/>researchers in country i publish about some topics that researchers in country j <lb/>do not (though this is, of course, usually a matter of degree rather than an issue <lb/>of presence and absence). This means that it is harder on average for a scientist in <lb/>country i to find a counterpart in country j that is working along a similar line of <lb/>research than it is for a scientist in country j to find a someone working on similar <lb/>problems in country i. This also means that it is easier to find a paper from country <lb/>j that cites a paper from country i than it is to find the reverse, assuming that <lb/>citations are more likely when two papers have the same subject matter. <lb/>In principle, when the information loss is high going from i to j, we say that the <lb/>similarity of i to j is low. When very little information is lost going from i to j, we <lb/>say that the similarity of i to j is high. Just as in the citation layer, z-scores are used <lb/>for edge weights, the only difference being that we take the negative here in the text <lb/>layer, as high information loss implies exactly the opposite relationship that high <lb/>citations imply. So, when we compare the multiplex layer Lcitation i,j , which measures <lb/>citational flow from country i to country j, with Ltext i,j , which captures how similar <lb/>country i is to country j, we use the transpose of L text to result in L T <lb/>text , where the <lb/>similarity of country j to country i given as L T <lb/>texti,j is equivalent to Ltext j,i . L T <lb/>text is used <lb/>in equation (1). We use this transpose because the more researchers in country <lb/>j cite researchers in country i, we posit that the work produced by researchers in <lb/>country j (that is, who is doing the citing and thus is giving the attention to the <lb/>work being done in country i with their citations) ought to be more similar to the <lb/>work produced in country i (that is, who is being cited and receiving the attention <lb/>from country j). Distortions thus ought to reflect either over-recognition or <lb/>under-recognition via attention (vis-à-vis citations) relative to the work being <lb/>done elsewhere. <lb/>The third layer, L distortion , is what we call the citational well (drawing on the idea <lb/>of gravity wells). This layer is constructed so that it will capture the difference <lb/>between the other two layers, as given by equation (1) and Fig. 1. This means <lb/>that every L distortioni,j represents the distortion in the citation flow from country <lb/>i to country j, relative to what we would expect on the basis of the similarity of <lb/>the text written by country i&apos;s scientists to that written by scientists in country j. <lb/>Also implied is that the sum of the distortion for country j relative to every other <lb/>country in the network-country j&apos;s in-degree in L distortion -represents the total <lb/>distortion in the citation flow to country j. <lb/>To illustrate how citational lensing can be applied, we use nearly 20 million <lb/>academic papers in nearly 150 fields and subfields from 1980 to 2012 in MAG, one <lb/>of the most extensive metadata repositories of academic publications. <lb/></body>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>927 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>
            <body>These data include metadata such as citations, along with the abstract text of <lb/>published research articles. We show how citational lensing can be used to <lb/>characterize changes in the international scientific hierarchy over time and how it <lb/>can be scaled to cover all of science. <lb/>MAG classifies journals into various fields, which provides a fairly reliable <lb/>reflection of disciplinary boundaries and allows for selection across a wide variety <lb/>of fields. MAG uses a six-tiered field classification ID scheme that is human <lb/>generated for the highest two levels. We primarily use the second-highest level, <lb/>which offers these more granular field divisions. So instead of using just &apos;physics&apos; , <lb/>we consider &apos;astrophysics&apos; and &apos;nuclear physics&apos; to be their own fields because they <lb/>have different citation practices. Fields are identified and defined for our purposes <lb/>as their field IDs in MAG, and the fields are itemized in Supplementary Table 1. <lb/>We classify these fields into four broad categories: (1) biomedical, behavioural <lb/>and ecological sciences; (2) engineering and computational sciences; (3) physical <lb/>and mathematical sciences; and (4) social sciences. We use no other sort of field <lb/>normalization. The population of journals in MAG increases considerably over <lb/>time, which may partly affect the representation of countries in our analysis. <lb/>L citation is assembled using the citation data in MAG. As mentioned above, <lb/>each Lcitation i,j holds the citation flow from country i to country j. Because citation <lb/>inflation 14 distorts the volume of cumulative citations in a field over time, <lb/>rendering temporal comparisons biased, we standardize and &apos;deflate&apos; the number <lb/>of citations received in years t + n to the equivalent number of citations that would <lb/>have been received in the year t that the paper was published. In essence, the <lb/>citations received in a future t + n year are converted into an exchange rate based <lb/>on the year the paper was published t, rendering comparing citations across time <lb/>less biased by volume. (In Supplementary Figs. 10-15, we rebuild our main figures <lb/>comparing the citation deflation method that we use here to two other conditions: <lb/>one that does not include any deflation and another that employs our own <lb/>deflation method focusing specifically on countries.) <lb/>L text is constructed using text from the abstracts and titles of each paper. This <lb/>has advantages over using the full texts of research papers, since some fields format <lb/>papers to emphasize methods over theory or vice versa, and others might have a <lb/>strict length criterion, in terms of word count or page length. Abstracts, however, <lb/>succinctly summarize the most important concepts in a paper. We restrict our <lb/>analysis to papers with English-only abstracts. (In Supplementary Figs. 22-27, <lb/>we rebuild our main figures comparing these English-only abstracts to those that <lb/>were subsequently translated from their original language into English by us using <lb/>Google Translate.) <lb/>We build both Lcitation i,j and L text using only those journals that have existed <lb/>in our data since 1980, the starting point of our analyses. (In Supplementary <lb/>Figs. 16-21, we rebuild our main figures including all journals irrespective of <lb/>their tenure in the data.) The important terms and phrases that represent ideas, <lb/>concepts and phenomena need to be efficiently extracted from abstract texts. So, <lb/>we construct each field&apos;s corpus in year t as a combination of unigrams, bigrams <lb/>and trigrams from every document&apos;s abstract, referred to as Field t . For our analyses <lb/>here, we use English-only abstracts to mitigate the risk of mistranslation. We also <lb/>translate non-English abstracts using a Python module called googletrans that <lb/>functions as an API with Google Translate and reconstruct our analyses, but our <lb/>conclusions are consistent with what we present here. We apply a phrase extraction <lb/>algorithm called RAKE (Rapid Automatic Keyword Extraction) to each abstract <lb/>to extract all important phrases and terms from unigrams through trigrams 35 . <lb/>RAKE extracts terms and phrases from abstracts by analysing the frequency of <lb/>each n-gram and its co-occurrences with other n-grams in the text. An advantage <lb/>of RAKE over other approaches is that it is domain independent, so it does not rely <lb/>on a pretrained corpus to identify what terms are important. We then compiled an <lb/>&apos;academic stop word&apos; list of common phrases used in academic writing based on <lb/>Coxhead 36 and removed them from the abstracts. <lb/>KLD compares probability distributions. To process the text of scientific <lb/>articles so that each country has its own probability distribution, we apply <lb/>NL-LDA models on abstracts from MAG publication abstracts to measure how <lb/>similar or dissimilar the phenomena studied by researchers in different countries <lb/>are 2-4 . We apply an NL-LDA model to each Field t corpus. This approach parses <lb/>the influence of countries on multi-authored, international papers, a staple of <lb/>many fields. We measure how similar individual countries&apos; unique national <lb/>signatures-or how strongly associated the terms found in a field&apos;s corpus in a year <lb/>are associated to researchers in some country x-are to one another. The NL-LDA <lb/>produces a matrix, φ Fieldt , where the rows are the n-grams in the corpus for Field t <lb/>defined as w m and the columns are the national signatures defined as C n . We <lb/>standardize each national signature (column) in φ Fieldt such that for each national <lb/>signature, we assign zero values to all terms that were not present in papers <lb/>authored from a particular country. (Our implementation of the NL-LDA model <lb/>assigns a very small non-zero value to all terms that are not present in documents <lb/>with a particular nation-label but are present in Field t .) As the national signatures <lb/>sum to 100%, we then renormalize each national signature after we convert the <lb/>associative probabilities of absent terms to zero so that the national signature still <lb/>sums to 100%. <lb/>We first validate the quality of the nation-labels produced by the NL-LDA using <lb/>topic cohesion scores, the standard measure for how distinct a topic is from other <lb/>topics derived from the same model. A cohesive topic forms a distinctive grouping <lb/>of its top n-grams that differentiates it from other topics. However, to date, no <lb/>equivalent approach exists to measure nation-label cohesion for a supervised <lb/>model like the NL-LDA in the same way as topic cohesion does for unsupervised <lb/>models like the LDA. This is because the number of appropriate topics extracted <lb/>from an LDA is variable and somewhat subjective, but the NL-LDA nation-labels <lb/>are nominally fixed. That said, not every country may produce enough published <lb/>papers in a year to produce meaningful results, so including every country in <lb/>our analyses without any filtering may not be prudent. We apply the umass topic <lb/>cohesion measure to the nation-labels in each NL-LDA model, where we compare <lb/>the document co-occurrences of each nation-label&apos;s top 25 strongest associated <lb/>terms from its national signature. Whereas with unsupervised LDA models, lower <lb/>scores indicate more distinct and cohesive topics, with NL-LDA models, the <lb/>opposite holds true: nation-labels with strong national signatures lead the way in <lb/>global science and have lexical usage that is more widespread throughout the field. <lb/>For each NL-LDA model, we convert these scores into percentile ranks, where <lb/>the nation-labels that are the most ubiquitous (such as the United States and in <lb/>later years China) are in the highest percentile (that is, they have lower coherence <lb/>scores) and less active countries are in the lowest percentile (that is, they have <lb/>higher coherence scores). For the results presented here, all of the nation-labels <lb/>are included in the analyses. In Supplementary Figs. 1-7, we rerun nearly all of the <lb/>figures presented here at the 25th and 75th percentiles. Our results broadly hold <lb/>despite the exclusion of nation-labels. <lb/>With these matrices, we measure how similar any country&apos;s subject matter is <lb/>to that of all other countries for some Field t . However, a standard similarity score <lb/>(like a cosine similarity) is not directed, and our aim is to understand how much <lb/>one country looks like another when reciprocation may not happen. We compare <lb/>every country to every other country in φ Fieldt and take the KLD of every column <lb/>in φ Fieldt to every other column, where each comparison is a weighted, directed link <lb/>that creates an international network of asymmetric text similarity. To calculate <lb/>this score, we take the two vectors for a country i and another country j, presented <lb/>as their national signature vectors c i and c j , respectively, to determine how similar <lb/>they are to each other: <lb/>KLD(ci ∥ cj) = <lb/>∑ <lb/>ci log <lb/>ci <lb/>cj <lb/>(2) <lb/>Here KLD measures how much information is lost by national signature c i <lb/>when approximated with the national signature from c j . In other words, the less <lb/>information that is lost by approximating c i with c j , the more similar c j is to c i . <lb/>From here, we construct 4,914 international networks of topic similarity across <lb/>nearly 150 academic fields and 33 years of data (that is, 1980 to 2012), defined as <lb/>KLD Fieldt (referred to in the results as L T <lb/>text ). <lb/>We create an upper bound for KLD in the following way: for each KLD <lb/>network, KLD Fieldt , we take the negative of its z-score, so that the lowest value <lb/>(that is, the lowest information loss and the most similar country dyad) is <lb/>normalized to be the largest value relative to all other edge weights in the <lb/>network (in terms of standard deviations). The dyad with the lowest raw <lb/>KLD score is thus the dyad where the least amount of information is lost by <lb/>approximating c i with c j , so that country i is highly aligned with country j. <lb/>This approach is advantageous as it renders comparison across networks <lb/>possible, particularly for extreme values. <lb/>Statistics and reproducibility. Our analyses were observational, and no statistical <lb/>method was used to predetermine sample size. <lb/>Reporting summary. Further information on research design is available in the <lb/>Nature Research Reporting Summary linked to this article. <lb/></body>

			<div type="availability">Data availability <lb/>All data that were used to create the figures are available on the Harvard Dataverse <lb/>at https://doi.org/10.7910/DVN/WCOINR. <lb/>Code availability <lb/>All code that was used to perform the analyses and to construct the figures is <lb/>available on the Harvard Dataverse at https://doi.org/10.7910/DVN/WCOINR. <lb/></div>

			<front>Received: 23 February 2021; Accepted: 11 April 2022; <lb/>Published online: 30 May 2022 <lb/>references <lb/></front>

			<listBibl>1. Doi, H., Heeren, A. &amp; Maurage, P. Scientific activity is a better predictor of <lb/>Nobel award chances than dietary habits and economic factors. PLoS ONE 9, <lb/>e92612 (2014). <lb/>2. Collyer, F. M. Global patterns in the publishing of academic knowledge: <lb/>Global North, Global South. Curr. Sociol. 66, 56-73 (2018). <lb/>3. A More Research-Intensive and Integrated European Research Area Science, <lb/>Technology and Competitiveness Key Figures Report 2008/2009 (EU <lb/>Commission, 2008); http://aei.pitt.edu/46028/ <lb/></listBibl>

			<note place="footnote">NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></note>

			<page>928 <lb/></page>

			<note place="headnote">Articles <lb/>NATure HuMAN BeHAviOur <lb/></note>

			<listBibl>4. Shu, F. et al. The role of Web of Science publications in China&apos;s tenure system. <lb/>Scientometrics 122, 1683-1695 (2020). <lb/>5. Nielsen, M. W. &amp; Andersen, J. P. Global citation inequality is on the rise. <lb/>Proc. Natl. Acad. Sci. USA 118, e2012208118 (2021). <lb/>6. Merton, R. K. The Matthew effect in science: the reward and communication <lb/>systems of science are considered. Science 159, 56-63 (1968). <lb/>7. Latour, B. Science in Action: How to Follow Scientists and Engineers Through <lb/>Society (Harvard Univ. Press, 1987). <lb/>8. Kullback, S. &amp; Leibler, R. A. On information and sufficiency. Ann. Math. Stat. <lb/>22, 79-86 (1951). <lb/>9. Ramage, D., Hall, D., Nallapati, R. &amp; Manning, C. D. Labeled LDA: a <lb/>supervised topic model for credit attribution in multi-labeled corpora. In <lb/>Proc. 2009 Conference on Empirical Methods in Natural Language Processing <lb/>Vol. 1 (Eds. Koehn, P. and Mihalcea, R.) 248-256 (Association for <lb/>Computational Linguistics, 2009). <lb/>10. Fortunato, S. et al. Science of science. Science 359, eaao0185 (2018). <lb/>11. Cohan, A., Feldman, S., Beltagy, I., Downey, D. &amp; Weld, D. SPECTER: <lb/>document-level representation learning using citation-informed transformers. <lb/>In Proc. 58th Annual Meeting of the Association for Computational Linguistics <lb/>(Eds. Jurafsky, D., Chai, J., et al.) 2270-2282 (Association for Computational <lb/>Linguistics, 2020); https://doi.org/10.18653/v1/2020.acl-main.207 <lb/>12. Vilhena, D. et al. Finding cultural holes: how structure and culture diverge in <lb/>networks of scholarly communication. Sociol. Sci. 1, 221-238 (2014). <lb/>13. Altmann, E. G., Dias, L. &amp; Gerlach, M. Generalized entropies and the <lb/>similarity of texts. J. Stat. Mech. 2017, 014002 (2017). <lb/>14. Petersen, A. M., Pan, R. K., Pammolli, F. &amp; Fortunato, S. Methods to account <lb/>for citation inflation in research evaluation. Res. Policy 48, 1855-1865 (2019). <lb/>15. Dias, L., Gerlach, M., Scharloth, J. &amp; Altmann, E. G. Using text analysis to <lb/>quantify the similarity and evolution of scientific disciplines. R. Soc. Open Sci. <lb/>5, 171545 (2018). <lb/>16. Lu, C. et al. Analyzing linguistic complexity and scientific impact. J. Informetr. <lb/>13, 817-829 (2019). <lb/>17. Glenisson, P., Glänzel, W., Janssens, F. &amp; De Moor, B. Combining full text and <lb/>bibliometric information in mapping scientific disciplines. Inf. Process. <lb/>Manage. 41, 1548-1572 (2005). <lb/>18. Janssens, F., Glänzel, W. &amp; De Moor, B. A hybrid mapping of information <lb/>science. Scientometrics 75, 607-631 (2008). <lb/>19. Dekker, D., Snijders, T. A. B. &amp; Krackhardt, D. Sensitivity of MRQAP <lb/>tests to collinearity and autocorrelation conditions. Psychometrika 72, <lb/>563-581 (2007). <lb/>20. Leydesdorff, L. &amp; Wagner, C. S. International collaboration in science and the <lb/>formation of a core group. J. Informetr. 2, 317-325 (2008). <lb/>21. Zelnio, R. Identifying the global core-periphery structure of science. <lb/>Scientometrics 91, 601-615 (2011). <lb/>22. Becker, R. A. &amp; Wilks, A. R. R version by R. Brownrigg. Enhancements by T. <lb/>P. Minka &amp; A. Deckmyn. maps: Draw geographical maps. R package v 3.4.0 <lb/>(2018). <lb/>23. Xie, Y., Zhang, C. &amp; Lai, Q. China&apos;s rise as a major contributor to science and <lb/>technology. Proc. Natl. Acad. Sci. USA 111, 9437-9442 (2014). <lb/>24. Zhou, P. &amp; Leydesdorff, L. The emergence of China as a leading nation in <lb/>science. Res. Policy 35, 83-104 (2006). <lb/>25. He, T., Zhang, J. &amp; Teng, L. Basic research in biochemistry and molecular <lb/>biology in China: a bibliometric analysis. Scientometrics 62, 249-259 (2005). <lb/>26. Kuhn, T. S. The Structure of Scientific Revolutions (Univ. of Chicago Press, <lb/>1962). <lb/>27. Gomez, C. J. &amp; Lazer, D. M. J. Clustering knowledge and dispersing abilities <lb/>enhances collective problem solving in a network. Nat. Commun. 10, <lb/>5146 (2019). <lb/>28. Annan, K. A challenge to the world&apos;s scientists. Science 299, 1485-1486 (2003). <lb/>29. Holmgren, M. &amp; Schnitzer, S. A. Science on the rise in developing countries. <lb/>PLoS Biol. 2, e1 (2004). <lb/>30. Monroe-White, T. &amp; Woodson, T. S. Inequalities in scholarly knowledge: <lb/>public value failures and their impact on global science. Afr. J. Sci. Technol. <lb/>Innov. Dev. 8, 178-186 (2016). <lb/>31. Tilly, C. Unequal access to scientific knowledge. J. Hum. Dev. 8, <lb/>245-258 (2007). <lb/>32. Moed, H. F., Burger, W. J. M., Frankfort, J. G. &amp; Van Raan, A. F. J. The use of <lb/>bibliometric data for the measurement of university research performance. <lb/>Res. Policy 14, 131-149 (1985). <lb/>33. Leydesdorff, L., Bornmann, L., Comins, J. A. &amp; Milojević, S. Citations: <lb/>indicators of quality? The impact fallacy. Front. Res. Metr. Anal. 1, <lb/>1-15 (2016). <lb/>34. Huang, F. Quality deficit belies the hype. Nature 564, S70-S71 (2018). <lb/>35. Rose, S., Engel, D., Cramer, N. &amp; Cowley, W. in Text Mining: Applications and <lb/>Theory (eds Berry, M. W. &amp; Kogan, J.) 1-20 (John Wiley &amp; Sons, 2010); <lb/>https://doi.org/10.1002/9780470689646.ch1 <lb/>36. Coxhead, A. A new academic word list. TESOL Q. 34, 213-238 (2000). <lb/></listBibl>

			<div type="acknowledgement">Acknowledgements <lb/>We thank D. McFarland, W. Powell, A. Goldberg, J. P. Alperin, B. L. Boerjesson, B. Keep, <lb/>L. Rodman, S. Muñoz-Najar Galvez, J. Sohn, E. Mäkinen, E. Evans, the UCLA workshop <lb/>on computational social science participants and the CUNY Faculty Fellowship <lb/>Publication Program spring 2020 participants (K. Chen, M. Dibello, N. Hougaard, <lb/>A. Lambert, E. Minei and M. Sanchez.), who all provided insightful and thoughtful <lb/>comments on various versions of this article. This work is supported by funding from <lb/>the NSF (award no. 2022395): &apos;Inequality in Global Scientific Research: Implications for <lb/>Novelty and Innovation&apos; PI: (CUNY-QC), C.J.G. Any opinions, findings, and conclusions <lb/>or recommendations expressed in this material are those of the author(s) and do not <lb/>necessarily reflect the views of the NSF. The funders had no role in study design, data <lb/>collection and analysis, decision to publish or preparation of the manuscript. <lb/></div>

			<div type="annex">Author contributions <lb/>C.J.G. conceptualized the initial project and its methodology, performed the analyses, <lb/>coordinated the project, and drafted, reviewed and edited the paper. A.C.H. contributed <lb/>to conceptually developing the project and its methodology and drafted, reviewed <lb/>and edited the paper. P.P. contributed to conceptually developing the project and its <lb/>methodology and reviewed the paper. <lb/>Competing interests <lb/>The authors declare no competing interests. <lb/></div>

			<front>Additional information <lb/>Supplementary information The online version contains supplementary material <lb/>available at https://doi.org/10.1038/s41562-022-01351-5. <lb/>Correspondence and requests for materials should be addressed to Charles J. Gomez. <lb/>Peer review information Nature Human Behaviour thanks Jens Peter Andersen and the <lb/>other, anonymous, reviewer(s) for their contribution to the peer review of this work. <lb/>Peer reviewer reports are available. <lb/>Reprints and permissions information is available at www.nature.com/reprints. <lb/>Publisher&apos;s note Springer Nature remains neutral with regard to jurisdictional claims in <lb/>published maps and institutional affiliations. <lb/>Open Access This article is licensed under a Creative Commons <lb/>Attribution 4.0 International License, which permits use, sharing, adap-<lb/>tation, distribution and reproduction in any medium or format, as long <lb/>as you give appropriate credit to the original author(s) and the source, provide a link to <lb/>the Creative Commons license, and indicate if changes were made. The images or other <lb/>third party material in this article are included in the article&apos;s Creative Commons license, <lb/>unless indicated otherwise in a credit line to the material. If material is not included in <lb/>the article&apos;s Creative Commons license and your intended use is not permitted by statu-<lb/>tory regulation or exceeds the permitted use, you will need to obtain permission directly <lb/>from the copyright holder. To view a copy of this license, visit http://creativecommons. <lb/>org/licenses/by/4.0/. <lb/>© The Author(s) 2022 <lb/>NAture HumAN BeHAviour | VOL 6 | JULY 2022 | 919-929 | www.nature.com/nathumbehav <lb/></front>

			<page>929 <lb/></page>

			<body>1 <lb/>nature research | reporting summary <lb/>April 2020 <lb/>Corresponding author(s): Charles J. Gomez <lb/>Last updated by author(s): Mar 3, 2022 <lb/>Reporting Summary <lb/>Nature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency <lb/>in reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist. <lb/>Statistics <lb/>For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section. <lb/>n/a Confirmed <lb/>The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement <lb/>A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly <lb/>The statistical test(s) used AND whether they are one-or two-sided <lb/>Only common tests should be described solely by name; describe more complex techniques in the Methods section. <lb/>A description of all covariates tested <lb/>A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons <lb/>A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) <lb/>AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals) <lb/>For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted <lb/>Give P values as exact values whenever suitable. <lb/>For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings <lb/>For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes <lb/>Estimates of effect sizes (e.g. Cohen&apos;s d, Pearson&apos;s r), indicating how they were calculated <lb/>Our web collection on statistics for biologists contains articles on many of the points above. <lb/>Software and code <lb/>Policy information about availability of computer code <lb/>Data collection Data are primarily from the publicly available Microsoft Academic Graph (MAG). <lb/>Data analysis <lb/>Data were analyzed, processed, and visualized using Python 3, R, and SQL. The data are stored on Athena (Amazon), queried and read in to <lb/>Python, analyzed with Python, and visualized with R. <lb/>For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and <lb/>reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code &amp; software for further information. <lb/>Data <lb/>Policy information about availability of data <lb/>All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: <lb/>-Accession codes, unique identifiers, or web links for publicly available datasets <lb/>-A list of figures that have associated raw data <lb/>-A description of any restrictions on data availability <lb/>All data derived from the Microsoft Academic Graph (e.g., corpora, NL-LDA models, CSV edgelists, etc.), along with all relevant code in Python 3 and R used to <lb/>analyze and visualize these data, will be made available at the Harvard Dataverse upon publication. <lb/></body>

			<page>2 <lb/></page>

			<note place="headnote">nature research | reporting summary <lb/></note>

			<div type="annex">April 2020 <lb/>Field-specific reporting <lb/>Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection. <lb/>Life sciences <lb/>Behavioural &amp; social sciences <lb/>Ecological, evolutionary &amp; environmental sciences <lb/>For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf <lb/>Behavioural &amp; social sciences study design <lb/>All studies must disclose on these points even when the disclosure is negative. <lb/>Study description <lb/>The data are quantitative and text-based. The text data were processed using nation-labeled LDA models and using MRQAP network <lb/>regression models. <lb/>Research sample <lb/>We selected 181 field IDs from the Microsoft Academic Graph (MAG) and read in all of the published paper metadata from papers <lb/>published between 1980 and 2015, inclusive. <lb/>Sampling strategy <lb/>No sample size calculations were performed. Fields in MAG are classified into a several-tier hierarchy that progresses deeper into <lb/>field sub-specialties. We selected fields from tier-1 which has the most representative set of fields without selecting from sub-<lb/>specialties. <lb/>Data collection <lb/>Data were procured from the publicly available Microsoft Academic Graph (MAG). <lb/>Timing <lb/>Our metadata in the Microsoft Academic Graph (MAG) extend to 2017. We stop our analyses in 2015 since the total number of <lb/>papers in 2017 drops compared to 2016, indicating that these data may not be fully complete. <lb/>Data exclusions <lb/>Only papers that had abstract text data were used in our analyses. <lb/>Non-participation <lb/>Participants were not used in this study. <lb/>Randomization <lb/>Randomization strategies were not relevant to this study. <lb/>Reporting for specific materials, systems and methods <lb/>We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, <lb/>system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. <lb/>Materials &amp; experimental systems <lb/>n/a Involved in the study <lb/>Antibodies <lb/>Eukaryotic cell lines <lb/>Palaeontology and archaeology <lb/>Animals and other organisms <lb/>Human research participants <lb/>Clinical data <lb/>Dual use research of concern <lb/>Methods <lb/>n/a Involved in the study <lb/>ChIP-seq <lb/>Flow cytometry <lb/>MRI-based neuroimaging </div>


	</text>
</tei>
